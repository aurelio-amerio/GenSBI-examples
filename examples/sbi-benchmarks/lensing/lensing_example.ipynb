{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Gravitational Lensing Example: CNN Embedding + Flow Matching\n",
                "\n",
                "This notebook demonstrates a Simulation-Based Inference (SBI) workflow for a Strong Lensing task. \n",
                "\n",
                "## Conceptual Overview\n",
                "\n",
                "We aim to infer the parameters $\\theta$ of a lensing system (e.g., lens mass, shear) given an observed image $x$.\n",
                "\n",
                "**Strategy:**\n",
                "1.  **Compression (VAE)**: We use a **Variational Autoencoder (VAE)** to compress the high-dimensional conditioning data (32x32 images) into a lower-dimensional latent representation.\n",
                "2.  **Inference (Flow Matching)**: We condition our inference model (a **Flux1** Flow Matching model) on this latent representation.\n",
                "\n",
                "The encoder is trained end-to-end with the flow matching model to optimize the inference objective."
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Configuration & Data Dimensions\n",
                "\n",
                "We use the configuration from `config_1a.yaml`.\n",
                "\n",
                "### Data Dimensions\n",
                "- **Observation ($\\\\theta$)**: The target of inference. It has **2 features** (parameters) and **1 channel**.\n",
                "- **Conditioning ($x$)**: Lensing images. **32x32 pixels** with **1 channel**.\n",
                "\n",
                "### Processing Pipeline\n",
                "The conditioning images go through several transformation steps before entering the inference model:\n",
                "\n",
                "1.  **VAE Encoder**: The 32x32x1 image is processed by the VAE encoder, which outputs a latent feature map of shape **8x8x16**.\n",
                "2.  **Patchification**: We apply standard Vision Transformer (ViT) patchification with 2x2 patches. \n",
                "    - Spatial dimension reduces by factor of 2: $8 \\to 4$.\n",
                "    - Channel dimension increases by factor of $2 \\times 2 = 4$: $16 \\to 64$.\n",
                "    - Resulting shape: **4x4x64**.\n",
                "3.  **Reshaping**: For the Transformer, we flatten the spatial dimensions.\n",
                "    - $4 \\times 4 = 16$ tokens.\n",
                "    - Each token has size **64**.\n",
                "    - Resulting array: **16x64**.\n",
                "\n",
                "The pipeline handles the initialization of condition IDs to represent the patched structure of the image."
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 1. Setup and Imports\n",
                "\n",
                "First, we set up the environment and import necessary libraries."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 1,
            "id": "c9f4414e",
            "metadata": {},
            "outputs": [],
            "source": [
                "# Check if running on Colab and install dependencies if needed\n",
                "try:\n",
                "    import google.colab\n",
                "    colab = True\n",
                "except ImportError:\n",
                "    colab = False\n",
                "\n",
                "if colab:\n",
                "    # Install required packages and clone the repository\n",
                "    %pip install --quiet \"gensbi[cuda12, examples] @ git+https://github.com/aurelio-amerio/GenSBI\"\n",
                "    !git clone --depth 1 https://github.com/aurelio-amerio/GenSBI-examples\n",
                "    %cd GenSBI-examples/examples/sbi-benchmarks/lensing"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import os\n",
                "\n",
                "if os.environ.get(\"JAX_PLATFORMS\") is None:\n",
                "    # os.environ[\"JAX_PLATFORMS\"] = \"cpu\"\n",
                "    os.environ[\"XLA_PYTHON_CLIENT_MEM_FRACTION\"] = \".90\"  # use 90% of GPU memory\n",
                "    os.environ[\"JAX_PLATFORMS\"] = \"cuda\"  # change to 'cpu' if no GPU is available\n",
                "\n",
                "import gensbi\n",
                "\n",
                "# base libraries\n",
                "import jax\n",
                "from jax import Array\n",
                "from jax import numpy as jnp\n",
                "import numpy as np\n",
                "from flax import nnx\n",
                "\n",
                "from tqdm import tqdm\n",
                "import gc\n",
                "\n",
                "# data loading\n",
                "import grain\n",
                "from datasets import load_dataset\n",
                "import yaml\n",
                "\n",
                "# plotting\n",
                "import matplotlib.pyplot as plt\n",
                "\n",
                "# gensbi\n",
                "from gensbi.recipes import ConditionalFlowPipeline\n",
                "from gensbi.recipes.flux1 import parse_flux1_params, parse_training_config\n",
                "from gensbi.recipes.utils import patchify_2d\n",
                "\n",
                "from gensbi.experimental.models.autoencoders import AutoEncoder2D, AutoEncoderParams\n",
                "from gensbi.experimental.recipes.vae_pipeline import parse_autoencoder_params\n",
                "from gensbi.models import Flux1Params, Flux1\n",
                "\n",
                "from gensbi.utils.plotting import plot_marginals\n",
                "\n",
                "from gensbi.diagnostics import LC2ST, plot_lc2st\n",
                "from gensbi.diagnostics import run_sbc, sbc_rank_plot\n",
                "from gensbi.diagnostics import run_tarp, plot_tarp\n",
                "\n",
                "from gensbi_examples.tasks import GravitationalLensing\n",
                "\n",
                "config_path = \"./config/config_1a.yaml\""
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 2. Helper Functions and Classes"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def normalize(batch, mean, std):\n",
                "    mean = jnp.asarray(mean, dtype=batch.dtype)\n",
                "    std = jnp.asarray(std, dtype=batch.dtype)\n",
                "    return (batch - mean) / std\n",
                "\n",
                "\n",
                "def unnormalize(batch, mean, std):\n",
                "    mean = jnp.asarray(mean, dtype=batch.dtype)\n",
                "    std = jnp.asarray(std, dtype=batch.dtype)\n",
                "    return batch * std + mean\n",
                "\n",
                "\n",
                "class LensingModel(nnx.Module):\n",
                "    \"\"\"\n",
                "    A combined model that first encodes the conditioning data (images) using a VAE,\n",
                "    and then passes the latent embedding to the SBI model (Flux).\n",
                "    \"\"\"\n",
                "    def __init__(self, vae, sbi_model):\n",
                "        self.vae = vae\n",
                "        self.sbi_model = sbi_model\n",
                "\n",
                "    def __call__(\n",
                "        self,\n",
                "        t: Array,\n",
                "        obs: Array,\n",
                "        obs_ids: Array,\n",
                "        cond: Array,\n",
                "        cond_ids: Array,\n",
                "        conditioned: bool | Array = True,\n",
                "        guidance: Array | None = None,\n",
                "        encoder_key=None,\n",
                "    ):\n",
                "\n",
                "        # first we encode the conditioning data\n",
                "        cond_latent = self.vae.encode(cond, encoder_key)\n",
                "        # patchify the cond_latent for the transformer\n",
                "        cond_latent = patchify_2d(cond_latent)\n",
                "\n",
                "        # then we pass to the sbi model\n",
                "        return self.sbi_model(\n",
                "            t=t,\n",
                "            obs=obs,\n",
                "            obs_ids=obs_ids,\n",
                "            cond=cond_latent,\n",
                "            cond_ids=cond_ids,\n",
                "            conditioned=conditioned,\n",
                "            guidance=guidance,\n",
                "        )"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 3. Data Loading\n",
                "\n",
                "We load the Lensing dataset."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "dim_obs = 2\n",
                "ch_obs = 1\n",
                "\n",
                "task = GravitationalLensing()\n",
                "\n",
                "df_train = task.df_train\n",
                "df_val = task.df_val\n",
                "df_test = task.df_test\n",
                "\n",
                "xs_mean = jnp.array([-1.1874731e-05], dtype=jnp.bfloat16).reshape(1, 1, 1)\n",
                "thetas_mean = jnp.array([0.5996428, 0.15998043], dtype=jnp.bfloat16).reshape(1, 2)\n",
                "\n",
                "xs_std = jnp.array([1.0440514], dtype=jnp.bfloat16).reshape(1, 1, 1)\n",
                "thetas_std = jnp.array([0.2886958, 0.08657552], dtype=jnp.bfloat16).reshape(1, 2)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "5a47c2fb",
            "metadata": {},
            "outputs": [],
            "source": [
                "# plot a sample\n",
                "x_o = df_test[\"xs\"][0][None, ...]\n",
                "x_o = normalize(jnp.array(x_o, dtype=jnp.bfloat16), xs_mean, xs_std)\n",
                "\n",
                "theta_true = df_test[\"thetas\"][0]  # already unnormalized"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "d67ea87e",
            "metadata": {},
            "outputs": [],
            "source": [
                "plt.imshow(x_o[0], cmap=\"gray\")\n",
                "plt.axis(\"off\")\n",
                "plt.savefig(\"img/lensing.png\", bbox_inches=\"tight\", dpi=300)\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "7df87168",
            "metadata": {},
            "source": [
                "<img src=\"https://raw.githubusercontent.com/aurelio-amerio/GenSBI-examples/refs/heads/main/examples/sbi-benchmarks/lensing/imgs/lensing.png\" alt=\"lensing\" width=\"500\">"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 4. Model Initialization"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "params_dict = parse_autoencoder_params(config_path)\n",
                "\n",
                "ae_params = AutoEncoderParams(\n",
                "    rngs=nnx.Rngs(0),\n",
                "    **params_dict,\n",
                ")\n",
                "\n",
                "# define the vae model\n",
                "vae_model = AutoEncoder2D(ae_params)\n",
                "\n",
                "# for the sake of the NPE, we delete the decoder model as it is not needed\n",
                "vae_model.Decoder1D = None\n",
                "# run the garbage collector to free up memory\n",
                "gc.collect()\n",
                "\n",
                "# now we define the NPE pipeline\n",
                "# get the latent dimensions from the autoencoder\n",
                "latent_dim1 = vae_model.latent_shape[1]\n",
                "latent_dim2 = vae_model.latent_shape[2]\n",
                "\n",
                "# After 2x2 patchification, dimensions are halved\n",
                "dim_cond_latent = (latent_dim1 // 2) * (latent_dim2 // 2)\n",
                "# Channels are multiplied by 4 (2x2)\n",
                "ch_cond_latent = vae_model.latent_shape[3] * 4\n",
                "\n",
                "print(f\"Original Latent Shape: {vae_model.latent_shape}\")\n",
                "print(f\"Conditioning Transformer Input: {dim_cond_latent} tokens of size {ch_cond_latent}\")\n",
                "\n",
                "params_dict_flux = parse_flux1_params(config_path)\n",
                "assert (\n",
                "    params_dict_flux[\"context_in_dim\"] == ch_cond_latent\n",
                "), \"Context dimension mismatch, got {} expected {}\".format(\n",
                "    params_dict_flux[\"context_in_dim\"], ch_cond_latent\n",
                ")\n",
                "\n",
                "params_flux = Flux1Params(\n",
                "    rngs=nnx.Rngs(0),\n",
                "    dim_obs=dim_obs,\n",
                "    dim_cond=dim_cond_latent,\n",
                "    **params_dict_flux,\n",
                ")\n",
                "\n",
                "model_sbi = Flux1(params_flux)\n",
                "\n",
                "model = LensingModel(vae_model, model_sbi)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 5. Pipeline Setup and Restoration"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "training_config = parse_training_config(config_path)\n",
                "\n",
                "with open(config_path, \"r\") as f:\n",
                "    config = yaml.safe_load(f)\n",
                "    batch_size = config[\"training\"][\"batch_size\"]\n",
                "    nsteps = config[\"training\"][\"nsteps\"]\n",
                "    multistep = config[\"training\"][\"multistep\"]\n",
                "    experiment = config[\"training\"][\"experiment_id\"]\n",
                "\n",
                "def split_data(batch):\n",
                "    obs = jnp.array(batch[\"thetas\"], dtype=jnp.bfloat16)\n",
                "    obs = normalize(obs, thetas_mean, thetas_std)\n",
                "    obs = obs.reshape(obs.shape[0], dim_obs, ch_obs)\n",
                "    cond = jnp.array(batch[\"xs\"], dtype=jnp.bfloat16)\n",
                "    cond = normalize(cond, xs_mean, xs_std)\n",
                "    cond = cond[..., None]\n",
                "    return obs, cond\n",
                "\n",
                "train_dataset_npe = (\n",
                "    grain.MapDataset.source(df_train).shuffle(42).repeat().to_iter_dataset()\n",
                ")\n",
                "\n",
                "performance_config = grain.experimental.pick_performance_config(\n",
                "    ds=train_dataset_npe,\n",
                "    ram_budget_mb=1024 * 8,\n",
                "    max_workers=None,\n",
                "    max_buffer_size=None,\n",
                ")\n",
                "\n",
                "train_dataset_npe = (\n",
                "    train_dataset_npe.batch(batch_size)\n",
                "    .map(split_data)\n",
                "    .mp_prefetch(performance_config.multiprocessing_options)\n",
                ")\n",
                "\n",
                "val_dataset_npe = (\n",
                "    grain.MapDataset.source(df_val)\n",
                "    .shuffle(42)\n",
                "    .repeat()\n",
                "    .to_iter_dataset()\n",
                "    .batch(256)\n",
                "    .map(split_data)\n",
                ")\n",
                "\n",
                "# Set checkpoint directory\n",
                "current_dir = os.getcwd()\n",
                "checkpoint_dir = os.path.join(current_dir, \"checkpoints\")\n",
                "training_config[\"checkpoint_dir\"] = checkpoint_dir\n",
                "\n",
                "pipeline_latent = ConditionalFlowPipeline(\n",
                "    model,\n",
                "    train_dataset_npe,\n",
                "    val_dataset_npe,\n",
                "    dim_obs=dim_obs,\n",
                "    dim_cond=(\n",
                "        latent_dim1,\n",
                "        latent_dim2,\n",
                "    ),  # we are workin in the latent space of the vae\n",
                "    ch_obs=ch_obs,\n",
                "    ch_cond=ch_cond_latent,  # conditioning is now in the latent space\n",
                "    training_config=training_config,\n",
                "    id_embedding_strategy=(\"absolute\", \"rope2d\"),\n",
                ")\n",
                "\n",
                "print(\"Restoring model...\")\n",
                "pipeline_latent.restore_model()\n",
                "print(\"Done!\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 6. Inference and Visualization\n",
                "\n",
                "We generate samples and visualize the posterior for a test observation."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "x_o = df_test[\"xs\"][0][None, ...]\n",
                "x_o = normalize(jnp.array(x_o, dtype=jnp.bfloat16), xs_mean, xs_std)\n",
                "x_o = x_o[..., None]\n",
                "\n",
                "theta_true = df_test[\"thetas\"][0]  # already unnormalized\n",
                "\n",
                "print(\"Sampling 100,000 samples...\")\n",
                "samples = pipeline_latent.sample_batched(\n",
                "    nnx.Rngs(0).sample(),\n",
                "    x_o,\n",
                "    100_000,\n",
                "    chunk_size=10_000,\n",
                "    encoder_key=jax.random.PRNGKey(1234),\n",
                ")\n",
                "\n",
                "res = samples[:, 0, :, 0]  # shape (num_samples, 1, 2, 1) -> (num_samples, 2)\n",
                "# unnormalize the results for plotting\n",
                "res_unnorm = unnormalize(res, thetas_mean, thetas_std)\n",
                "\n",
                "plot_marginals(res_unnorm, true_param=theta_true, gridsize=30)\n",
                "plt.title(f\"Lensing Samples (Exp {experiment})\")\n",
                "# plt.savefig(f\"imgs/lensing_samples_conf{experiment}.png\", dpi=100, bbox_inches=\"tight\")\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "b0eecdcb",
            "metadata": {},
            "source": [
                "<img src=\"https://raw.githubusercontent.com/aurelio-amerio/GenSBI-examples/refs/heads/main/examples/sbi-benchmarks/lensing/imgs/lensing_samples_conf1.png\" alt=\"lensing\" width=\"500\">"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 7. Diagnostics\n",
                "\n",
                "You should run several diagnostics to validate the quality of the posterior estimation.\n",
                "We leave as an excercise the implementation of these diagnostics (or take a look at the training script: [`train-lensing.py`](https://github.com/aurelio-amerio/GenSBI-examples/blob/main/examples/sbi-benchmarks/lensing/train-lensing.py)).\n",
                "\n",
                "Below are the results of the diagnostics for the lensing example.\n",
                "\n",
                "> **Note:** <br>\n",
                "> Running these tests is slow, and requires a GPU."
            ]
        },
        {
            "cell_type": "markdown",
            "id": "f0b67588",
            "metadata": {},
            "source": [
                "**SBC**: <br>\n",
                "<img src=\"https://raw.githubusercontent.com/aurelio-amerio/GenSBI-examples/refs/heads/main/examples/sbi-benchmarks/lensing/imgs/lensing_sbc_conf1.png\" alt=\"lensing\" width=\"500\">\n",
                "\n",
                "**Marginal coverage**: <br>\n",
                "<img src=\"https://raw.githubusercontent.com/aurelio-amerio/GenSBI-examples/refs/heads/main/examples/sbi-benchmarks/lensing/imgs/lensing_marginal_coverage_conf1.png\" alt=\"lensing\" width=\"800\">\n",
                "\n",
                "**TARP**: <br>\n",
                "<img src=\"https://raw.githubusercontent.com/aurelio-amerio/GenSBI-examples/refs/heads/main/examples/sbi-benchmarks/lensing/imgs/lensing_tarp_conf1.png\" alt=\"lensing\" width=\"800\">\n",
                "\n",
                "**L-C2ST**: <br>\n",
                "<img src=\"https://raw.githubusercontent.com/aurelio-amerio/GenSBI-examples/refs/heads/main/examples/sbi-benchmarks/lensing/imgs/lensing_lc2st_conf1.png\" alt=\"lensing\" width=\"400\">"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "d5c4ad22",
            "metadata": {},
            "source": [
                "### Conclusions\n",
                "The SBC test tells us that the posterior for the first dimension is reasonable, while the second dimension is slightly skewed. <br>\n",
                "The marginal coverage test tells us that the the posterior for the first dimension is slightly underconfident while the second dimension is slightly overconfident. <br>\n",
                "The TARP test tells us that the posterior is overall well-calibrated, but the model is slightly overconfident in the region around the peak of the distribution ($\\alpha$<0.6). <br>\n",
                "The L-C2ST test tells us that a local classifier cannot distinguish samples drawan from the true joint distributions, and samples drawn from the model, meaning that we can't reject the hypothesis that the model is well calibrated. <br>\n",
                "\n",
                "Final remark: The model in this example has been trained in a \"fast\" configuration, as an example, and can be further refined by optimizing the model parameters and training configuration."
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "gensbi",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.12.11"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 5
}
