{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Gravitational Lensing Example: CNN Embedding + Flow Matching\n",
                "\n",
                "This notebook demonstrates a Simulation-Based Inference (SBI) workflow for a Strong Lensing task. \n",
                "\n",
                "## Conceptual Overview\n",
                "\n",
                "We aim to infer the parameters $\\theta$ of a lensing system (e.g., lens mass, shear) given an observed image $x$.\n",
                "\n",
                "**Strategy:**\n",
                "1.  **Compression (VAE)**: We use a **Variational Autoencoder (VAE)** to compress the high-dimensional conditioning data (32x32 images) into a lower-dimensional latent representation.\n",
                "2.  **Inference (Flow Matching)**: We condition our inference model (a **Flux1** Flow Matching model) on this latent representation.\n",
                "\n",
                "The encoder is trained end-to-end with the flow matching model to optimize the inference objective."
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Configuration & Data Dimensions\n",
                "\n",
                "We use the configuration from `config_1a.yaml`.\n",
                "\n",
                "### Data Dimensions\n",
                "- **Observation ($\\\\theta$)**: The target of inference. It has **2 features** (parameters) and **1 channel**.\n",
                "- **Conditioning ($x$)**: Lensing images. **32x32 pixels** with **1 channel**.\n",
                "\n",
                "### Processing Pipeline\n",
                "The conditioning images go through several transformation steps before entering the inference model:\n",
                "\n",
                "1.  **VAE Encoder**: The 32x32x1 image is processed by the VAE encoder, which outputs a latent feature map of shape **8x8x16**.\n",
                "2.  **Patchification**: We apply standard Vision Transformer (ViT) patchification with 2x2 patches. \n",
                "    - Spatial dimension reduces by factor of 2: $8 \\to 4$.\n",
                "    - Channel dimension increases by factor of $2 \\times 2 = 4$: $16 \\to 64$.\n",
                "    - Resulting shape: **4x4x64**.\n",
                "3.  **Reshaping**: For the Transformer, we flatten the spatial dimensions.\n",
                "    - $4 \\times 4 = 16$ tokens.\n",
                "    - Each token has size **64**.\n",
                "    - Resulting array: **16x64**.\n",
                "\n",
                "The pipeline handles the initialization of condition IDs to represent the patched structure of the image."
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 1. Setup and Imports\n",
                "\n",
                "First, we set up the environment and import necessary libraries."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 1,
            "metadata": {},
            "outputs": [],
            "source": [
                "import os\n",
                "\n",
                "if os.environ.get(\"JAX_PLATFORMS\") is None:\n",
                "    os.environ[\"JAX_PLATFORMS\"] = \"cpu\"\n",
                "    os.environ[\"XLA_PYTHON_CLIENT_MEM_FRACTION\"] = \".90\"  # use 90% of GPU memory\n",
                "    # os.environ[\"JAX_PLATFORMS\"] = \"cuda\"  # change to 'cpu' if no GPU is available\n",
                "\n",
                "import gensbi\n",
                "\n",
                "# base libraries\n",
                "import jax\n",
                "from jax import Array\n",
                "from jax import numpy as jnp\n",
                "import numpy as np\n",
                "from flax import nnx\n",
                "\n",
                "from tqdm import tqdm\n",
                "import gc\n",
                "\n",
                "# data loading\n",
                "import grain\n",
                "from datasets import load_dataset\n",
                "import yaml\n",
                "\n",
                "# plotting\n",
                "import matplotlib.pyplot as plt\n",
                "\n",
                "# gensbi\n",
                "from gensbi.recipes import ConditionalFlowPipeline\n",
                "from gensbi.recipes.flux1 import parse_flux1_params, parse_training_config\n",
                "from gensbi.recipes.utils import patchify_2d\n",
                "\n",
                "from gensbi.experimental.models.autoencoders import AutoEncoder2D, AutoEncoderParams\n",
                "from gensbi.experimental.recipes.vae_pipeline import parse_autoencoder_params\n",
                "from gensbi.models import Flux1Params, Flux1\n",
                "\n",
                "from gensbi.utils.plotting import plot_marginals\n",
                "\n",
                "from gensbi.diagnostics import LC2ST, plot_lc2st\n",
                "from gensbi.diagnostics import run_sbc, sbc_rank_plot\n",
                "from gensbi.diagnostics import run_tarp, plot_tarp\n",
                "\n",
                "config_path = \"./config/config_1a.yaml\""
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 2. Helper Functions and Classes"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 2,
            "metadata": {},
            "outputs": [],
            "source": [
                "def normalize(batch, mean, std):\n",
                "    mean = jnp.asarray(mean, dtype=batch.dtype)\n",
                "    std = jnp.asarray(std, dtype=batch.dtype)\n",
                "    return (batch - mean) / std\n",
                "\n",
                "\n",
                "def unnormalize(batch, mean, std):\n",
                "    mean = jnp.asarray(mean, dtype=batch.dtype)\n",
                "    std = jnp.asarray(std, dtype=batch.dtype)\n",
                "    return batch * std + mean\n",
                "\n",
                "\n",
                "class LensingModel(nnx.Module):\n",
                "    \"\"\"\n",
                "    A combined model that first encodes the conditioning data (images) using a VAE,\n",
                "    and then passes the latent embedding to the SBI model (Flux).\n",
                "    \"\"\"\n",
                "    def __init__(self, vae, sbi_model):\n",
                "        self.vae = vae\n",
                "        self.sbi_model = sbi_model\n",
                "\n",
                "    def __call__(\n",
                "        self,\n",
                "        t: Array,\n",
                "        obs: Array,\n",
                "        obs_ids: Array,\n",
                "        cond: Array,\n",
                "        cond_ids: Array,\n",
                "        conditioned: bool | Array = True,\n",
                "        guidance: Array | None = None,\n",
                "        encoder_key=None,\n",
                "    ):\n",
                "\n",
                "        # first we encode the conditioning data\n",
                "        cond_latent = self.vae.encode(cond, encoder_key)\n",
                "        # patchify the cond_latent for the transformer\n",
                "        cond_latent = patchify_2d(cond_latent)\n",
                "\n",
                "        # then we pass to the sbi model\n",
                "        return self.sbi_model(\n",
                "            t=t,\n",
                "            obs=obs,\n",
                "            obs_ids=obs_ids,\n",
                "            cond=cond_latent,\n",
                "            cond_ids=cond_ids,\n",
                "            conditioned=conditioned,\n",
                "            guidance=guidance,\n",
                "        )"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 3. Data Loading\n",
                "\n",
                "We load the Lensing dataset."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 3,
            "metadata": {},
            "outputs": [],
            "source": [
                "dim_obs = 2\n",
                "ch_obs = 1\n",
                "repo_name = \"aurelio-amerio/SBI-benchmarks\"\n",
                "task_name = \"lensing\"\n",
                "\n",
                "dataset = load_dataset(repo_name, task_name).with_format(\"numpy\")\n",
                "\n",
                "df_train = dataset[\"train\"]\n",
                "df_val = dataset[\"validation\"]\n",
                "df_test = dataset[\"test\"]\n",
                "\n",
                "xs_mean = jnp.array([-1.1874731e-05], dtype=jnp.bfloat16).reshape(1, 1, 1)\n",
                "thetas_mean = jnp.array([0.5996428, 0.15998043], dtype=jnp.bfloat16).reshape(1, 2)\n",
                "\n",
                "xs_std = jnp.array([1.0440514], dtype=jnp.bfloat16).reshape(1, 1, 1)\n",
                "thetas_std = jnp.array([0.2886958, 0.08657552], dtype=jnp.bfloat16).reshape(1, 2)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 5,
            "id": "5a47c2fb",
            "metadata": {},
            "outputs": [],
            "source": [
                "# plot a sample\n",
                "x_o = df_test[\"xs\"][0][None, ...]\n",
                "x_o = normalize(jnp.array(x_o, dtype=jnp.bfloat16), xs_mean, xs_std)\n",
                "\n",
                "theta_true = df_test[\"thetas\"][0]  # already unnormalized"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 12,
            "id": "d67ea87e",
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYUAAAGFCAYAAAASI+9IAAAAOnRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjEwLjMsIGh0dHBzOi8vbWF0cGxvdGxpYi5vcmcvZiW1igAAAAlwSFlzAAAPYQAAD2EBqD+naQAAF4JJREFUeJzt3EuMHnQZLvD/9DYzvUzb6b1CC1JKubQLUjEhiCaChiDGACpoJMREWRETExdu3LBVjAsTiIYENYISwQAiqASIKDFC1RRLikTuvU47vUyn08vMnN2bs+v/OTnNueT3Wz958vW7zNNv8b0Ds7Ozsw0AWmtz/k8/AAD+72EUAChGAYBiFAAoRgGAYhQAKEYBgGIUACjzeoPbt2+Pit94443u7JIlS6LuxOTkZJQfGBjozi5cuDDqnpiY6M4ODQ1F3atXr47y+/btO2+P5ezZs93ZefO634KttdaS31qm3efTyZMno/ycOf3/XxsdHY26Z2ZmurPJ5yHNJ//G1rLPT2utTU9Pd2cXL14cdSeGh4ej/EUXXdSd3bVrV9S9e/fuc2Z8UwCgGAUAilEAoBgFAIpRAKAYBQCKUQCgGAUAilEAoBgFAIpRAKAMzHYek1m5cmVUnNydSW8fnThxojt7+vTpqHt8fLw7u2bNmqg7kT7u5cuXR/nk7szU1FTUnbw+6c2ZI0eOdGfT52TBggVR/vDhw1H+fEnuQbWWfZZPnToVdX/xi1/szj722GNRd/r6bNu2rTv79ttvR93JZ+LYsWNRd3Kz6ejRo1F3z/0o3xQAKEYBgGIUAChGAYBiFAAoRgGAYhQAKEYBgGIUAChGAYDSfeZi/fr1UfHw8HB3dvXq1VH3yZMnu7NvvPFG1L1hw4bubHLKo7Xsp/HpT/rTcwTJczg4OHjeutPzKclzePz48aj7fJ5bWbduXdR94MCB7mz6+iRnFNL34eTkZJRPzJ8/P8on5z/ScyvJ6Yr0ZE1ygubMmTNRd89pFt8UAChGAYBiFAAoRgGAYhQAKEYBgGIUAChGAYBiFAAoRgGAYhQAKNnxnkBy/2b37t1Rd3KnZNGiReete2JiIupObpqkt4zSmzNLly7tzu7fvz/qXrZsWXc2vU80MzPTnU1vGZ09ezbKp/2J5PbRpk2bou6xsbHubHJrqrXsrtLQ0FDUnfxNaa21FStWdGeTW0atZXfP0htpyb2p9GZTD98UAChGAYBiFAAoRgGAYhQAKEYBgGIUAChGAYBiFAAoRgGA0v376/RcxKFDh7qzs7OzUXdy6mD58uVRd5JPTzScOXOmO5ueXFi9enWUT04GJOcCUslZhNay92H6HKZnF5JTJMlpidayMyRp98jISHf27rvvjrofeOCB8/I4WstPbiSnKy6++OKoO3nOBwYGou758+d3Z9PnpIdvCgAUowBAMQoAFKMAQDEKABSjAEAxCgAUowBAMQoAFKMAQDEKAJTu20dz5mT7kdzLSW7ItJbdEhkfH4+6T58+3Z0dHByMupN/56pVq85bd2vZ67l3796oe926dd3Z9HEn94wWLlwYdSf3oFrLXv/087NkyZLubPq4jxw50p19+eWXo+7R0dHubPJZay2/Y3bbbbd1Zx988MGoe8GCBd3Z9MbThx9+2J1Nb9L18E0BgGIUAChGAYBiFAAoRgGAYhQAKEYBgGIUAChGAYBiFAAoA7Ozs7M9wUsvvTQqHhsb684mP+lvrbUTJ050Z5Of3bfW2rJly7qzx48fj7qTcxHz5nVfIGmt5f/O5DTC3Llzo+5Dhw51Z9NzHslpif3790fdQ0NDUX7x4sXd2eQ8R2utHTt2rDs7MzMTdSf59DlZu3ZtdzY5t9Faa5OTk1E+eX3SMyTJqZ30PZ48L8nf2dZaO3jw4DkzvikAUIwCAMUoAFCMAgDFKABQjAIAxSgAUIwCAMUoAFCMAgDFKABQum8fbdq0KSqenp7uzqa3dc6cOdOdPXXqVNSd3DRJbqu0lt1sGh4ejrqT5yTNp3eVktc+fX0WLlzYnT2ft3Jay+4CpXeyku4DBw5E3du3b+/O7ty5M+o+fPhwdzb5rLWW30ibmJg4b49lZGSkO5vcGWst+7y5fQTAeWUUAChGAYBiFAAoRgGAYhQAKEYBgGIUAChGAYBiFAAo83qD6U+1k5/1pz9fT05ApD9fHx8f784uWrQo6u68KNJayx93+vosXbq0O5v8pL+17OzCzMxM1J2cf7jsssui7uQ8R2vZY7/00kuj7jfffLM7u3z58qg7eV527NgRdSevz8033xx1P/vss1F+xYoV3dnkBE1rrc2b1/2ns1100UVRd3L6JTn70ss3BQCKUQCgGAUAilEAoBgFAIpRAKAYBQCKUQCgGAUAilEAoBgFAMrAbOdBno0bN0bFyb2c5N5Qa9mdkjVr1kTdyT2jycnJqDu5B3X48OGoO73DNH/+/O5sel8luQuzefPmqHtwcLA7e/XVV0fd6f2otWvXdmc/+OCDqHvfvn3nJdtaa++//3539tixY1H3ggULurNnz56NupObQK1lr2dys6m11r72ta91Z997772o+8UXX+zOXnjhhVH33//+93NmfFMAoBgFAIpRAKAYBQCKUQCgGAUAilEAoBgFAIpRAKAYBQCKUQCgdB+pOXLkSFS8Z8+e7mxyQ6a11ubOndudTe+lDAwMdGcnJiai7kRyO6q1/PbR9PR0dza9UbN48eLzkm2tta1bt3Zn09c+uQfVWmuHDh3qzq5bty7qHh0dPS/Z1lo7ePBgd/aWW26Juh977LHubHo7LP13JneyvvWtb0XdP/rRj7qz27dvj7pvuumm7uzy5cuj7h6+KQBQjAIAxSgAUIwCAMUoAFCMAgDFKABQjAIAxSgAUIwCAKX7zMXs7GxUvHnz5u5s8rP71lpbsmRJd/bAgQNR90c/+tHu7OnTp6PuhQsXdmeHh4ej7sOHD0f55N95/PjxqDs56ZCcrWittTNnznRnX3/99ag7PbuQvJ7p2ZJrrrmmO3vttddG3cnn5xe/+EXUvWXLlu7sjh07ou70XMS8ed1/3trf/va3qDs5cbJ+/fqoOznj8/Of/zzqvv/++8+Z8U0BgGIUAChGAYBiFAAoRgGAYhQAKEYBgGIUAChGAYBiFAAoRgGA0n0cZPHixVFxcnNo0aJFUXfyWAYHB6PusbGx7uycOdmmTk1NdWfT52T16tVRPrnbdMUVV0TdyeuT3o9Kbh+lN5t27doV5ZPnfP78+VH3xo0bu7Ppc/jWW291Zy+77LKo+7XXXuvO3n333VF3crOptda+//3vd2e/853vRN2vvPJKd/aJJ56IutesWdOdXbFiRdTdwzcFAIpRAKAYBQCKUQCgGAUAilEAoBgFAIpRAKAYBQCKUQCgdJ+5OHr0aFQ8MDDQnU1POhw7dqw7e/Dgwaj7kksu6c6++eabUffmzZu7s3v27Im6U8mJjv3790fdN9xwQ3c2eZ+01tq8ed1v2eh90lp+yiV5b6VnSA4dOtSdHR4ejrqTcxGvvvpq1H3PPfd0Z59++umoe/fu3VE+eR8++OCDUXdybuXOO++Mun/96193Z9MTJz18UwCgGAUAilEAoBgFAIpRAKAYBQCKUQCgGAUAilEAoBgFAIpRAKAMzM7OzvYE161bFxXPzMx0Z0dGRqLuU6dOdWfT2zpJPvk3ttba4OBgdza9NZXev0lu8aT3VT7xiU90Z9esWRN1J15++eUo/9Zbb0X5lStXdmcXLFgQdV933XXd2Ysvvjjq/uY3v9md3b59e9Td+eektXb+3+NTU1Pd2fT1Sd63O3bsiLqTz0/6Hh8fHz9nxjcFAIpRAKAYBQCKUQCgGAUAilEAoBgFAIpRAKAYBQCKUQCgzOsNJqclWmvtwgsv7M6+9957UfcFF1zQnZ2cnIy6k9MV6XOSnNCYnp4+b92ttfbPf/6zO3vVVVdF3Tt37uzOpqcLktc+ORfQWn7K5frrr+/OfuxjH4u6r7jiiu7sfffdF3V/8pOf7M4mn+PWstfnmWeeibrffvvt8/ZYTpw4EXW/9NJL3dmtW7dG3cnpim9/+9tRdw/fFAAoRgGAYhQAKEYBgGIUAChGAYBiFAAoRgGAYhQAKEYBgGIUACgDs7Ozsz3B9P5Ncrun8yGUiYmJ7uzixYuj7uRxJ4+jtdZGR0e7s+ktljR/9uzZ7mxyD6q11rZt29adTW/rfO973+vOvvjii1H3vn37ovxXv/rV7mx652fOnP7/rx05ciTq3rVrV3f2tttui7qT1/6hhx6Kuv/yl79E+eT1PH78eNR9ww03dGf/9Kc/Rd0rV67szqZ/g955551zZnxTAKAYBQCKUQCgGAUAilEAoBgFAIpRAKAYBQCKUQCgGAUAilEAoHTfPrr88suj4snJye5scueltexOydDQUNQ9PDzcnR0fH4+6V61a1Z09duxY1J3eJ5o7d2539vbbb4+6H3/88e7sihUrou5NmzZ1Z5Pnu7XWvvKVr0T5xP79+6P8r371q+7szp07o+7Nmzd3Z1955ZWo++Mf/3h3ds+ePVH3ggULovxbb73VnZ2amoq6R0ZGurPr16+PusfGxrqza9eujbp7Xk/fFAAoRgGAYhQAKEYBgGIUAChGAYBiFAAoRgGAYhQAKEYBgDKvN3j27Nnz9iBWrlwZ5ZOTDulP4xPLly+P8slzmPyMvrX8jMLSpUu7sy+88ELUffjw4e7s/Pnzo+69e/d2ZycmJqLu//znP1F+enq6O3vixImo+x//+Ed3duvWrVH3smXLurOjo6NR99VXX92dffXVV6Pu5HG3lp2sufLKK6Pul156qTu7cOHCqPtTn/pUd/bZZ5+Nunv4pgBAMQoAFKMAQDEKABSjAEAxCgAUowBAMQoAFKMAQDEKABSjAEAZmJ2dne0Jbtq0KSpO7sKkN2qSGyhr166Nut97773u7OnTp6Pu5A5Tei/l+PHjUT65OTQ2NhZ1J8/5kSNHou477rijO/vnP/856k6fw6mpqe7shg0bou7Pf/7z3dnnnnsu6n7nnXe6s5/97Gej7qeeeqo7m35+5s6dG+Wvvfba7uzGjRuj7t/+9rfd2aGhoag7eR+mr89DDz10zoxvCgAUowBAMQoAFKMAQDEKABSjAEAxCgAUowBAMQoAFKMAQOk+c5Gei7j11lu7sy+88ELUnfwMfHJyMuo+depUd3bdunVR98DAQHd2ZmYm6t67d2+U37ZtW3f2iiuuiLpfeeWV7uxVV10Vdb/55pvd2fHx8ag7Of3RWmv33ntvd/b3v/991P3aa691Z1esWBF1J+/Dzj8PJfls3nzzzVH3k08+GeUTc+Zk/z/esmVLd/Zf//pX+nC6pe/Z/fv3nzPjmwIAxSgAUIwCAMUoAFCMAgDFKABQjAIAxSgAUIwCAMUoAFCMAgCl+/bRhg0bouLk5tDg4GDUfebMme5scueltewGSnLnpbXWRkZGurNf+MIXou6nnnoqyifP+cTExHnr/vSnPx11J+/Dhx9+OOpO73tdcskl3dmnn3466k7uGV122WVRd3JXa/fu3VF38tpff/31Ufcf//jHKJ983lLJDa70ffXBBx90Z9evXx91v/vuu+fM+KYAQDEKABSjAEAxCgAUowBAMQoAFKMAQDEKABSjAEAxCgCU7jMXmzZtiopXrVrVnf3www+j7uRn+suWLYu6Dxw40J29/fbbo+4nnniiO7tt27aoe8+ePVF+y5Yt3dmlS5dG3Tt37uzOHjp0KOoeGhrqzk5PT0fdJ06ciPKLFi3qzo6NjUXdGzdu7M5ecMEFUfe///3v7mznn4f/JYsXL47y6VmZ9PVMLF++vDub/n0bHR3tzqYngnrOlvimAEAxCgAUowBAMQoAFKMAQDEKABSjAEAxCgAUowBAMQoAFKMAQJnXGxwYGIiKk1s8aXeSv/XWW6PuRx55pDv73//+N+pOHnd6tyf10ksvdWfPnj0bdc+fP787++Uvfznq/t3vftedTe7TtNbavHndH4fWWmuXX355d/aNN96IupPXP/38XHfddd3Z4eHhqPuOO+7ozj766KNR944dO6J8ciPt5MmTUXdyyyr9LM+dO7c7m96D6uGbAgDFKABQjAIAxSgAUIwCAMUoAFCMAgDFKABQjAIAxSgAULp/15/89Lq11g4ePNidveWWW6Lu3bt3d2f/8Ic/RN1bt27tzu7atSvqTp7DtHvRokVRft26dd3ZiYmJqDv5Wf/KlSuj7uTswv79+6Pue+65J8r/9Kc/7c6mpw6S0yLp65Oc85gzJ/t/43PPPdedTU80LFmyJMqfPn06yidGRka6s4sXL4669+7d251NX5+uzv/tjQD8P8soAFCMAgDFKABQjAIAxSgAUIwCAMUoAFCMAgDFKABQjAIAZWB2dna2J3jjjTdGxcl9oqNHj0bdzz//fHf2G9/4RtQ9Pj7enf3MZz4Tdf/1r3/tzs6fPz/q3r59e5T/yEc+0p195JFHou6FCxd2Z2dmZqLu5L2yefPmqLvzo1DeeeedKJ9I7hMdOHAg6k5en/SzuWHDhvPWPTU1FeWT91Z62y3pTt9XyT2w5O9Va33vFd8UAChGAYBiFAAoRgGAYhQAKEYBgGIUAChGAYBiFAAoRgGAYhQAKN0HVgYHB6PiO++8szv7zDPPRN233nprd3bRokVRd3IDJb05k9xKevTRR6Pua665Jsrfd9993dm9e/dG3Zs2berOLl26NOpevnx5d/b111+PupcsWRLlk9s96b9zcnKyO3vRRRdF3RMTE93ZY8eORd1jY2Pd2WXLlkXdBw8ejPJz5vT/nze5N9RaayMjI93Z/fv3R93T09Pd2eHh4ai7h28KABSjAEAxCgAUowBAMQoAFKMAQDEKABSjAEAxCgAUowBAGZidnZ3tCV588cVR8enTp7uzX/rSl6Luxx9/vDt74403Rt1PP/10dzb5N7bWWudT3Vpr7aabboq6n3/++Sif/Kx/3759UffNN9/cnX322Wej7jVr1nRnjxw5EnUvXLgwyicnHYaGhqLu5HzBXXfdFXU/+OCD3dnR0dGoO3lOxsfHo+6tW7dG+eR9u379+qj73Xff7c4mZytay07zDAwMRN1vv/32OTO+KQBQjAIAxSgAUIwCAMUoAFCMAgDFKABQjAIAxSgAUIwCAMUoAFC6bx9deeWVUfEFF1zQnd2yZUvU/bOf/aw7Ozk5GXUnN1COHz8edSe3j9auXRt1HzhwIMonN1O+/vWvR90PP/xwdzZ5Tlprbd68ed3Zu+++O+r+8Y9/HOWXLVvWnU1vcP3yl7/szqZ3yZK7PTMzM1F3cpsq7R4cHIzyp06d6s6eOXMm6k7eh3Pnzo26k5tqc+Zk/6/vee19UwCgGAUAilEAoBgFAIpRAKAYBQCKUQCgGAUAilEAoBgFAEr3mYsVK1ZExfPnz+/ODg8PR90nT57szi5YsCDqvuuuu7qz999/f9S9atWq7uzixYuj7vScx5133tmd/c1vfhN1J6/PvffeG3X/4Ac/6M6mJwAWLVoU5YeGhrqzR48ejbqTEw3J42ittUOHDnVnk/dsa62dOHGiOzs6Ohp179+/P8onpyjSz9vU1FR3Nn0fjo2NdWfTv8t79uw5Z8Y3BQCKUQCgGAUAilEAoBgFAIpRAKAYBQCKUQCgGAUAilEAoBgFAEr37aNLL700Kk5uoMydOzfqPnPmzHnrTqxduzbKT09Pd2eT56+17GZTa6395Cc/6c4md6xaa2316tXd2fRm08zMTHd2YmIi6l62bFmUT27xpHeVzqfk9tHSpUuj7uQ5HB8fj7rTx5Lcj0rudbWW3VRL3+MjIyPd2Q8//DDqPnbs2DkzvikAUIwCAMUoAFCMAgDFKABQjAIAxSgAUIwCAMUoAFCMAgBlXm9wdHQ0Ku68ntFay84/tNbavHndDzvKtpb9xPyDDz44b93Jz+hba+2BBx6I8qdPn+7Opica3n///e7snDnZ/0s+97nPdWeffPLJqHvPnj1R/oc//GF39rvf/W7UPTU11Z1dtWpV1J28nj1nEf5nQ0ND3dn0s5m8Z1vL/q6k3cn7duHChVF3Ijn70ss3BQCKUQCgGAUAilEAoBgFAIpRAKAYBQCKUQCgGAUAilEAoBgFAMrAbHKkCID/r/mmAEAxCgAUowBAMQoAFKMAQDEKABSjAEAxCgAUowBA+R/O7yOJCgi6dwAAAABJRU5ErkJggg==",
                        "text/plain": [
                            "<Figure size 640x480 with 1 Axes>"
                        ]
                    },
                    "metadata": {},
                    "output_type": "display_data"
                }
            ],
            "source": [
                "plt.imshow(x_o[0], cmap=\"gray\")\n",
                "plt.axis(\"off\")\n",
                "plt.savefig(\"lensing.png\", bbox_inches=\"tight\", dpi=300)\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 4. Model Initialization"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "params_dict = parse_autoencoder_params(config_path)\n",
                "\n",
                "ae_params = AutoEncoderParams(\n",
                "    rngs=nnx.Rngs(0),\n",
                "    **params_dict,\n",
                ")\n",
                "\n",
                "# define the vae model\n",
                "vae_model = AutoEncoder2D(ae_params)\n",
                "\n",
                "# for the sake of the NPE, we delete the decoder model as it is not needed\n",
                "vae_model.Decoder1D = None\n",
                "# run the garbage collector to free up memory\n",
                "gc.collect()\n",
                "\n",
                "# now we define the NPE pipeline\n",
                "# get the latent dimensions from the autoencoder\n",
                "latent_dim1 = vae_model.latent_shape[1]\n",
                "latent_dim2 = vae_model.latent_shape[2]\n",
                "\n",
                "# After 2x2 patchification, dimensions are halved\n",
                "dim_cond_latent = (latent_dim1 // 2) * (latent_dim2 // 2)\n",
                "# Channels are multiplied by 4 (2x2)\n",
                "ch_cond_latent = vae_model.latent_shape[3] * 4\n",
                "\n",
                "print(f\"Original Latent Shape: {vae_model.latent_shape}\")\n",
                "print(f\"Conditioning Transformer Input: {dim_cond_latent} tokens of size {ch_cond_latent}\")\n",
                "\n",
                "params_dict_flux = parse_flux1_params(config_path)\n",
                "assert (\n",
                "    params_dict_flux[\"context_in_dim\"] == ch_cond_latent\n",
                "), \"Context dimension mismatch, got {} expected {}\".format(\n",
                "    params_dict_flux[\"context_in_dim\"], ch_cond_latent\n",
                ")\n",
                "\n",
                "params_flux = Flux1Params(\n",
                "    rngs=nnx.Rngs(0),\n",
                "    dim_obs=dim_obs,\n",
                "    dim_cond=dim_cond_latent,\n",
                "    **params_dict_flux,\n",
                ")\n",
                "\n",
                "model_sbi = Flux1(params_flux)\n",
                "\n",
                "model = LensingModel(vae_model, model_sbi)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 5. Pipeline Setup and Restoration"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "training_config = parse_training_config(config_path)\n",
                "\n",
                "with open(config_path, \"r\") as f:\n",
                "    config = yaml.safe_load(f)\n",
                "    batch_size = config[\"training\"][\"batch_size\"]\n",
                "    nsteps = config[\"training\"][\"nsteps\"]\n",
                "    multistep = config[\"training\"][\"multistep\"]\n",
                "    experiment = config[\"training\"][\"experiment_id\"]\n",
                "\n",
                "def split_data(batch):\n",
                "    obs = jnp.array(batch[\"thetas\"], dtype=jnp.bfloat16)\n",
                "    obs = normalize(obs, thetas_mean, thetas_std)\n",
                "    obs = obs.reshape(obs.shape[0], dim_obs, ch_obs)\n",
                "    cond = jnp.array(batch[\"xs\"], dtype=jnp.bfloat16)\n",
                "    cond = normalize(cond, xs_mean, xs_std)\n",
                "    cond = cond[..., None]\n",
                "    return obs, cond\n",
                "\n",
                "train_dataset_npe = (\n",
                "    grain.MapDataset.source(df_train).shuffle(42).repeat().to_iter_dataset()\n",
                ")\n",
                "\n",
                "performance_config = grain.experimental.pick_performance_config(\n",
                "    ds=train_dataset_npe,\n",
                "    ram_budget_mb=1024 * 8,\n",
                "    max_workers=None,\n",
                "    max_buffer_size=None,\n",
                ")\n",
                "\n",
                "train_dataset_npe = (\n",
                "    train_dataset_npe.batch(batch_size)\n",
                "    .map(split_data)\n",
                "    .mp_prefetch(performance_config.multiprocessing_options)\n",
                ")\n",
                "\n",
                "val_dataset_npe = (\n",
                "    grain.MapDataset.source(df_val)\n",
                "    .shuffle(42)\n",
                "    .repeat()\n",
                "    .to_iter_dataset()\n",
                "    .batch(256)\n",
                "    .map(split_data)\n",
                ")\n",
                "\n",
                "training_config[\"checkpoint_dir\"] = (\n",
                "    \"/lhome/ific/a/aamerio/data/github/GenSBI-examples/examples/sbi-benchmarks/lensing/npe_v1a/checkpoints\"\n",
                ")\n",
                "\n",
                "pipeline_latent = ConditionalFlowPipeline(\n",
                "    model,\n",
                "    train_dataset_npe,\n",
                "    val_dataset_npe,\n",
                "    dim_obs=dim_obs,\n",
                "    dim_cond=(\n",
                "        latent_dim1,\n",
                "        latent_dim2,\n",
                "    ),  # we are workin in the latent space of the vae\n",
                "    ch_obs=ch_obs,\n",
                "    ch_cond=ch_cond_latent,  # conditioning is now in the latent space\n",
                "    training_config=training_config,\n",
                "    id_embedding_strategy=(\"absolute\", \"rope2d\"),\n",
                ")\n",
                "\n",
                "print(\"Restoring model...\")\n",
                "pipeline_latent.restore_model()\n",
                "print(\"Done!\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 6. Inference and Visualization\n",
                "\n",
                "We generate samples and visualize the posterior for a test observation."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "x_o = df_test[\"xs\"][0][None, ...]\n",
                "x_o = normalize(jnp.array(x_o, dtype=jnp.bfloat16), xs_mean, xs_std)\n",
                "x_o = x_o[..., None]\n",
                "\n",
                "theta_true = df_test[\"thetas\"][0]  # already unnormalized\n",
                "\n",
                "print(\"Sampling 100,000 samples...\")\n",
                "samples = pipeline_latent.sample_batched(\n",
                "    nnx.Rngs(0).sample(),\n",
                "    x_o,\n",
                "    100_000,\n",
                "    chunk_size=10_000,\n",
                "    encoder_key=jax.random.PRNGKey(1234),\n",
                ")\n",
                "\n",
                "res = samples[:, 0, :, 0]  # shape (num_samples, 1, 2, 1) -> (num_samples, 2)\n",
                "# unnormalize the results for plotting\n",
                "res_unnorm = unnormalize(res, thetas_mean, thetas_std)\n",
                "\n",
                "plot_marginals(res_unnorm, true_param=theta_true, gridsize=30)\n",
                "plt.title(f\"Lensing Samples (Exp {experiment})\")\n",
                "# plt.savefig(f\"lensing_samples_conf{experiment}.png\", dpi=100, bbox_inches=\"tight\")\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 7. Diagnostics\n",
                "\n",
                "We run several diagnostics to validate the quality of the posterior estimation."
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### TARP (Test of Accuracy and Reliability of Posterior)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# # split in thetas and xs\n",
                "thetas_ = np.array(df_test[\"thetas\"])[:200]\n",
                "xs_ = np.array(df_test[\"xs\"])[:200]\n",
                "\n",
                "thetas_ = normalize(jnp.array(thetas_, dtype=jnp.bfloat16), thetas_mean, thetas_std)\n",
                "xs_ = normalize(jnp.array(xs_, dtype=jnp.bfloat16), xs_mean, xs_std)\n",
                "xs_ = xs_[..., None]\n",
                "\n",
                "num_posterior_samples = 1000\n",
                "\n",
                "print(\"Sampling for TARP...\")\n",
                "posterior_samples_ = pipeline_latent.sample_batched(\n",
                "    jax.random.PRNGKey(42),\n",
                "    xs_,\n",
                "    num_posterior_samples,\n",
                "    chunk_size=20,\n",
                "    encoder_key=jax.random.PRNGKey(1234),\n",
                ")\n",
                "\n",
                "thetas = thetas_.reshape(thetas_.shape[0], -1)\n",
                "xs = xs_.reshape(xs_.shape[0], -1)\n",
                "\n",
                "posterior_samples = posterior_samples_.reshape(\n",
                "    posterior_samples_.shape[0], posterior_samples_.shape[1], -1\n",
                ")\n",
                "\n",
                "ecp, alpha = run_tarp(\n",
                "    thetas,\n",
                "    posterior_samples,\n",
                "    references=None,  # will be calculated automatically.\n",
                ")\n",
                "\n",
                "plot_tarp(ecp, alpha)\n",
                "# plt.savefig(\n",
                "#     f\"lensing_tarp_v1a_conf{experiment}.png\", dpi=100, bbox_inches=\"tight\"\n",
                "# )  # uncomment to save the figure\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### SBC (Simulation-Based Calibration)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "ranks, dap_samples = run_sbc(thetas, xs, posterior_samples)\n",
                "\n",
                "f, ax = sbc_rank_plot(ranks, num_posterior_samples, plot_type=\"hist\", num_bins=20)\n",
                "# plt.savefig(\n",
                "#     f\"lensing_sbc_v1a_conf{experiment}.png\", dpi=100, bbox_inches=\"tight\"\n",
                "# )  # uncomment to save the figure\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### LC2ST (Local Classifier 2-Sample Test)\n",
                "This tests if the posterior samples are distinguishable from the true parameters."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "thetas_ = np.array(df_test[\"thetas\"])[:10_000]\n",
                "xs_ = np.array(df_test[\"xs\"])[:10_000]\n",
                "\n",
                "thetas_ = normalize(jnp.array(thetas_, dtype=jnp.bfloat16), thetas_mean, thetas_std)\n",
                "xs_ = normalize(jnp.array(xs_, dtype=jnp.bfloat16), xs_mean, xs_std)\n",
                "xs_ = xs_[..., None]\n",
                "\n",
                "num_posterior_samples = 1\n",
                "\n",
                "posterior_samples_ = pipeline_latent.sample(\n",
                "    jax.random.PRNGKey(42),\n",
                "    x_o=xs_,\n",
                "    nsamples=xs_.shape[0],\n",
                "    encoder_key=jax.random.PRNGKey(1234),\n",
                ")\n",
                "\n",
                "thetas = thetas_.reshape(thetas_.shape[0], -1)\n",
                "xs = xs_.reshape(xs_.shape[0], -1)\n",
                "posterior_samples = posterior_samples_.reshape(posterior_samples_.shape[0], -1)\n",
                "\n",
                "# Train the L-C2ST classifier.\n",
                "lc2st = LC2ST(\n",
                "    thetas=thetas[:-1],\n",
                "    xs=xs[:-1],\n",
                "    posterior_samples=posterior_samples[:-1],\n",
                "    classifier=\"mlp\",\n",
                "    num_ensemble=1,\n",
                ")\n",
                "\n",
                "_ = lc2st.train_under_null_hypothesis()\n",
                "_ = lc2st.train_on_observed_data()\n",
                "\n",
                "x_o = xs_[-1:]  # Take the last observation as observed data.\n",
                "theta_o = thetas_[-1:]  # True parameter for the observed data.\n",
                "\n",
                "post_samples_star = pipeline_latent.sample(\n",
                "    jax.random.PRNGKey(42), x_o, nsamples=10_000\n",
                ")\n",
                "\n",
                "x_o = x_o.reshape(1, -1)\n",
                "post_samples_star = np.array(\n",
                "    post_samples_star.reshape(post_samples_star.shape[0], -1)\n",
                ")\n",
                "\n",
                "fig, ax = plot_lc2st(\n",
                "    lc2st,\n",
                "    post_samples_star,\n",
                "    x_o,\n",
                ")\n",
                "# plt.savefig(\n",
                "#     f\"lensing_lc2st_v1a_conf{experiment}.png\", dpi=100, bbox_inches=\"tight\"\n",
                "# )  # uncomment to save the figure\n",
                "plt.show()"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "gensbi",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.12.11"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 5
}
