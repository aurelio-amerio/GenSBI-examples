{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "640892b2",
   "metadata": {},
   "source": [
    "# My First Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7f88520",
   "metadata": {},
   "source": [
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/aurelio-amerio/GenSBI-examples/blob/main/examples/getting_started/my_first_model.ipynb)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "539ae5cb",
   "metadata": {},
   "source": [
    "This guide will walk you through creating and training your first simulation-based inference model using GenSBI. We will cover the essential steps, from defining a simulator to training a neural density estimator."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7228374e",
   "metadata": {},
   "source": [
    "As a frist step, make sure gensbi is installed in your environment. If you haven't done so yet, please refer to the [Installation Guide](/getting_started/installation) before proceeding, or simply run:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b45ff30e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# step 1: install packages\n",
    "%pip install \"GenSBI[cuda12,examples,validation] @ git+https://github.com/aurelio-amerio/GenSBI.git\" --extra-index-url https://download.pytorch.org/whl/cpu"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be2f6b93",
   "metadata": {},
   "source": [
    "> <br>\n",
    "> Important: <br><br>\n",
    "> If you are using Colab, restart the runtime after installation to ensure all packages are properly loaded. <br>\n",
    "> <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad8df57c",
   "metadata": {},
   "source": [
    "Next it is convenient to download the GenSBI-examples package, which contains several example notebooks and checkpoints, including this one. You can do so by running:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "553688b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# step 2: clone the examples repository\n",
    "!git clone https://github.com/aurelio-amerio/GenSBI-examples.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8eac9d3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# step 3: cd into the examples folder\n",
    "%cd GenSBI-examples"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e97c4a04",
   "metadata": {},
   "source": [
    "Then we need to import all the necessary modules from GenSBI and other libraries. If you don't have a gpu available, you can set the device to \"cpu\" instead of \"cuda\", but training will be slower.\n",
    "\n",
    "If you are getting some errors relating to missing packages, restart the notebook kernel, and run step 3 again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10ac5993",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Set JAX backend (use 'cuda' for GPU, 'cpu' otherwise)\n",
    "os.environ[\"JAX_PLATFORMS\"] = \"cuda\"\n",
    "\n",
    "import grain\n",
    "import numpy as np\n",
    "import jax\n",
    "from jax import numpy as jnp\n",
    "from numpyro import distributions as dist\n",
    "from flax import nnx\n",
    "\n",
    "from gensbi.recipes import Flux1FlowPipeline\n",
    "from gensbi.models import Flux1Params\n",
    "\n",
    "from gensbi.utils.plotting import plot_marginals\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "torch.manual_seed(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26b0ab07",
   "metadata": {},
   "source": [
    "## The simulator"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93b2249b",
   "metadata": {},
   "source": [
    " The first step in simulation-based inference is to define a simulator function that generates data given parameters. In this example, we will create a simple simulator that generates data from a Gaussian distribution with a mean defined by the parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ca46c8e",
   "metadata": {},
   "source": [
    "The simulator takes in 3 parameters (theta) and returns 3 observations (xs). \n",
    "\n",
    "In the context of posterior density estimation, the `theta` parameters are the observations (what we want to model) and the `xs` are the conditions (the data we condition on, which we use to detemrine `theta`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a43dcbc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "obs_dim = 3 # dimension of the observation (theta), that is the simulator input shape\n",
    "cond_dim = 3 # dimension of the condition (xs), that is the simulator output shape\n",
    "joint_dim = obs_dim + cond_dim # dimension of the joint (theta, xs), useful later\n",
    "\n",
    "def _simulator(key, thetas):\n",
    "\n",
    "    xs = thetas + 1 + jax.random.normal(key, thetas.shape) * 0.1\n",
    "\n",
    "    thetas = thetas[..., None]\n",
    "    xs = xs[..., None]\n",
    "\n",
    "    # when making a dataset for the joint pipeline, thetas need to come first\n",
    "    data = jnp.concatenate([thetas, xs], axis=1)\n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcebf82a",
   "metadata": {},
   "source": [
    "Now we define a prior distribution over the parameters. For simplicity, we will use a uniform prior over a specified range."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fedf995",
   "metadata": {},
   "outputs": [],
   "source": [
    "theta_prior = dist.Uniform(\n",
    "    low=jnp.array([-2.0, -2.0, -2.0]), high=jnp.array([2.0, 2.0, 2.0])\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ee0f0aa",
   "metadata": {},
   "source": [
    "For simpliciy, we define a simulator which samples from the prior internally."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d937abc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def simulator(key, nsamples):\n",
    "    theta_key, sample_key = jax.random.split(key, 2)\n",
    "    thetas = theta_prior.sample(theta_key, (nsamples,))\n",
    "\n",
    "    return _simulator(sample_key, thetas)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "700b7ccf",
   "metadata": {},
   "source": [
    "## The dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef9dcc67",
   "metadata": {},
   "source": [
    "We create a dataset by running the simulator multiple times with parameters sampled from the prior distribution. This dataset will be used to train the neural density estimator.\n",
    "\n",
    "GenSBI can work with any dataset that provides an iterator to obtain couples of (parameters, observations). \n",
    "For numerical efficiency and ease of use, it is convenient to create a Jax-based datset using `grain`, for high efficiency data-loading and prefetching."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39505671",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define your training and validation datasets.\n",
    "train_data = simulator(jax.random.PRNGKey(0), 10_000)\n",
    "val_data = simulator(jax.random.PRNGKey(1), 2000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fd2a51c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# utility function to split data into observations and conditions\n",
    "def split_obs_cond(data):\n",
    "    return data[:, :obs_dim], data[:, obs_dim:]  # assuming first dim_obs are obs, last dim_cond are cond"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32f34cd8",
   "metadata": {},
   "source": [
    "We create a grain dataset with batch size = 128. The larger the batch size, the more stable the training.\n",
    "\n",
    "Adjust according to your hardware capabilities, e.g. GPU memory (try experimenting with 256, 512, 1024, etc).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f406808",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "\n",
    "train_dataset_grain = (\n",
    "    grain.MapDataset.source(np.array(train_data))\n",
    "    .shuffle(42)\n",
    "    .repeat()\n",
    "    .to_iter_dataset()\n",
    "    .batch(batch_size)\n",
    "    .map(split_obs_cond)\n",
    "    .mp_prefetch() # If you use prefetching in a .py script, make sure your python script is thread safe, see https://docs.python.org/3/library/multiprocessing.html\n",
    ")\n",
    "\n",
    "val_dataset_grain = (\n",
    "    grain.MapDataset.source(np.array(val_data))\n",
    "    .shuffle(42)\n",
    "    .repeat()\n",
    "    .to_iter_dataset()\n",
    "    .batch(batch_size)\n",
    "    .map(split_obs_cond)\n",
    "    .mp_prefetch() \n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c678015",
   "metadata": {},
   "source": [
    "These datasets are innfinite dataloaders, meaning that they will keep providing data as long as needed.\n",
    "You can get samples from the dataset using:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba1af81a",
   "metadata": {},
   "outputs": [],
   "source": [
    "iter_dataset = iter(train_dataset_grain)\n",
    "obs,cond = next(iter_dataset)  # returns a batch of (observations, conditions)\n",
    "print(obs.shape, cond.shape)  # should print (batch_size, obs_dim, 1), (batch_size, cond_dim, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5ac9e7d",
   "metadata": {},
   "source": [
    "## The Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67ded857",
   "metadata": {},
   "source": [
    "We define a `Flux1` model and pipeline. `Flux1` is a versatile transformer-based architecture suitable for various (complex) SBI tasks. Although for this problem a simpler architecture would suffice, we use `Flux1` to illustrate how to set up the main components of a GenSBI model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3f352d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the model parameters\n",
    "params = Flux1Params(\n",
    "    in_channels=1, # each observation/condition feature has only one channel (the value itself)\n",
    "    vec_in_dim=None,\n",
    "    context_in_dim=1, \n",
    "    mlp_ratio=3, # default value\n",
    "    num_heads=2, # number of transformer heads\n",
    "    depth=4, # number of double-stream transformer blocks\n",
    "    depth_single_blocks=8, # number of single-stream transformer blocks\n",
    "    axes_dim=[ 10,], # number of features per transformer head\n",
    "    qkv_bias=True, # default\n",
    "    obs_dim=obs_dim, # dimension of the observation (theta)\n",
    "    cond_dim=cond_dim, # dimension of the condition (xs)\n",
    "    theta=10*joint_dim, # dimension of the ROPE embedding space. A good rule of thumb is to set it to 10 times the joint dimension\n",
    "    rngs=nnx.Rngs(default=42), # random number generator seed\n",
    "    param_dtype=jnp.float32, # data type of the model parameters. \n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60cc6d7f",
   "metadata": {},
   "source": [
    "For the sake of running this example, we will "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13c5b558",
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_dir = os.path.join(os.getcwd(), \"examples/getting_started/checkpoints\")\n",
    "\n",
    "training_config = Flux1FlowPipeline._get_default_training_config()\n",
    "training_config[\"checkpoint_dir\"] = checkpoint_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb5b6f67",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Intantiate the pipeline\n",
    "pipeline = Flux1FlowPipeline(\n",
    "    train_dataset_grain,\n",
    "    val_dataset_grain,\n",
    "    obs_dim,\n",
    "    cond_dim,\n",
    "    params=params,\n",
    "    training_config=training_config,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e67fc0ab",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7f242be",
   "metadata": {},
   "source": [
    "Now we train the model using the defined pipeline. We specify the number of epochs and the random seed for reproducibility."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ea17843",
   "metadata": {},
   "outputs": [],
   "source": [
    "rngs = nnx.Rngs(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "521a5ec3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# uncomment to train the model\n",
    "# loss_history = pipeline.train(\n",
    "#     rngs, nsteps=5000, save_model=False\n",
    "# )  # if you want to save the model, set save_model=True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76273927",
   "metadata": {},
   "source": [
    "If you don't want to waste time retraining the model, you can directly load a pre-trained model for this example using this code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97bbb461",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline.restore_model(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02560e97",
   "metadata": {},
   "source": [
    "## Sampling from the posterior"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f8fbffe",
   "metadata": {},
   "source": [
    "In order to sample from the posterior distribution given new observations, we use the trained model's `sample` method. We provide the observation for which we want to reconstruct the posterior, and specify the number of samples we want to draw from the posterior."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d904ee23",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_sample = simulator(jax.random.PRNGKey(20), 1) # the observation for which we want to reconstruct the posterior\n",
    "\n",
    "true_theta = new_sample[:, :obs_dim, :]  # The input used for the simulation, AKA the true value\n",
    "x_o = new_sample[:, obs_dim:, :]  # The observation from the simulation for which we want to reconstruct the posterior"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b57a751e",
   "metadata": {},
   "source": [
    "Now we sample from the posterior:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84c45bd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "samples = pipeline.sample(rngs.sample(), x_o, nsamples=100_000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36eff93a",
   "metadata": {},
   "source": [
    "Once we have the samples, we display the marginal distributions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45f1e724",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_marginals(\n",
    "    np.array(samples[..., 0]), gridsize=30, true_param=np.array(true_theta[0, :, 0]), range = [(1, 3), (1, 3), (-0.6, 0.5)]\n",
    ")\n",
    "plt.savefig(\"flux1_flow_pipeline_marginals.png\", dpi=100, bbox_inches=\"tight\") # uncomment to save the figure\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a18204b",
   "metadata": {},
   "source": [
    "![flux1_flow_pipeline_marginals.png](https://raw.githubusercontent.com/aurelio-amerio/GenSBI-examples/refs/heads/main/examples/getting_started/flux1_flow_pipeline_marginals.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f50b90c2",
   "metadata": {},
   "source": [
    "## Next steps"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "461d3864",
   "metadata": {},
   "source": [
    "Congratulations! You have successfully created and trained your first simulation-based inference model using GenSBI. You can now experiment with different simulators, priors, and neural density estimators to explore more complex inference tasks.\n",
    "\n",
    "For more examples, please refer to the [Examples Section](/examples) of the GenSBI documentation.\n",
    "\n",
    "As a step forward, you might want to explore how to validate the performance of your trained model using techniques such as simulation-based calibration (SBC) or coverage plots. These methods help assess the quality of the inferred posterior distributions and ensure that your model is providing accurate uncertainty estimates."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42900c2b",
   "metadata": {},
   "source": [
    "## Posterior calibration tests"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "913cf3b5",
   "metadata": {},
   "source": [
    "In this section we perform posterior calibration tests using Simulation-Based Calibration (SBC), Targeted At Random Parameters (TARP) and L-C2ST methods to evaluate the quality of our trained model's posterior estimates.\n",
    "\n",
    "GenSBI does not provide built-in functions for SBC and TARP, but we can leverage the `sbi` package to perform these tests. GenSBI includes a compatibility layer that allows us to use `sbi`'s validation tools seamlessly with GenSBI models.\n",
    "In order to install `sbi`, as well as the compatibility layer, we can use the `GenSBI-validation` package.\n",
    "\n",
    "The following section demonstrates how to perform these posterior calibration tests using the `sbi` package in conjunction with our trained GenSBI model, and it draws strong inspiration from the `sbi` documentation.\n",
    "\n",
    "> **Note:** When rerunning this notebook, the plots for the folllowing tests may differ slightly due to the stochastic nature of the sampling process. In jax, random number generation is based on explicit random keys, while in torch it relies on a global random state, making it harder to enforce consistency across different runs. Due to this nature, it is not possible to explicitly set the random seed for sampling in the compatibility layer, beside at initialization time. However, the overall trends and conclusions drawn from the tests should remain consistent across different runs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba10e64d",
   "metadata": {},
   "source": [
    "For a full overiew of posterior calibration tests, refer to the [sbi documentation](https://sbi.readthedocs.io/en/latest/how_to_guide.html#diagnosticsl). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ddd0c9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "from gensbi_validation import PosteriorWrapper\n",
    "from sbi.analysis.plot import sbc_rank_plot\n",
    "from sbi.diagnostics import check_sbc, check_tarp, run_sbc, run_tarp\n",
    "from sbi.analysis.plot import plot_tarp\n",
    "\n",
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecd03af8",
   "metadata": {},
   "source": [
    "We wrap the pipeline posterior into a `PosteriorWrapper`, which provides an interface compatible with `sbi`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cbcae3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "posterior = PosteriorWrapper(pipeline, rngs=nnx.Rngs(1234))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6702c6a",
   "metadata": {},
   "source": [
    "We sample 200 new observations from the simulator to perform the calibration tests.\n",
    "It is instrumental that we use a seed different from the one used during training to avoid biased results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26468d52",
   "metadata": {},
   "outputs": [],
   "source": [
    "key = jax.random.PRNGKey(1234)\n",
    "# sample the dataset\n",
    "test_data = simulator(jax.random.PRNGKey(1), 200)\n",
    "\n",
    "# split in thetas and xs\n",
    "thetas = test_data[:, :obs_dim, :] # (200, 3, 1)\n",
    "xs = test_data[:, obs_dim:, :] # (200, 3, 1)\n",
    "\n",
    "# flatten the dataset. sbi expects 2D arrays of shape (num_samples, features), while our data is 3D of shape (num_samples, dim, channels).\n",
    "# we reshape a sample of size (dim, channels) into a vector of size (dim * channels)\n",
    "thetas = posterior._ravel(thetas) # (200, 3)\n",
    "xs = posterior._ravel(xs) # (200, 3)\n",
    "\n",
    "# convert to torch tensors\n",
    "thetas = torch.Tensor(np.array(thetas))\n",
    "xs = torch.Tensor(np.array(xs))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56f1ad74",
   "metadata": {},
   "source": [
    "### SBC"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ada81029",
   "metadata": {},
   "source": [
    "SBC allows you to evaluate whether individual marginals of the posterior are, on average across many observations (prior predictive samples) too narrow, too wide, or skewed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee959adc",
   "metadata": {},
   "outputs": [],
   "source": [
    "ranks, dap_samples = run_sbc(thetas, xs, posterior)\n",
    "check_stats = check_sbc(ranks, thetas, dap_samples, 1_000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "659ce692",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(check_stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efb2620f",
   "metadata": {},
   "outputs": [],
   "source": [
    "f, ax = sbc_rank_plot(ranks, 1_000, plot_type=\"hist\", num_bins=20)\n",
    "plt.savefig(\"flux1_flow_pipeline_sbc.png\", dpi=100, bbox_inches=\"tight\") # uncomment to save the figure\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26f5d683",
   "metadata": {},
   "source": [
    "![flux1_flow_pipeline_sbc.png](https://raw.githubusercontent.com/aurelio-amerio/GenSBI-examples/refs/heads/main/examples/getting_started/flux1_flow_pipeline_sbc.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ea5098e",
   "metadata": {},
   "source": [
    "All of the bars fall within the expected uniform distribution, thus we cannot reject the hypothesis that the posterior marginals are calibrated."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de1b8d73",
   "metadata": {},
   "source": [
    "See the SBI tutorial [https://sbi.readthedocs.io/en/latest/how_to_guide/16_sbc.html](https://sbi.readthedocs.io/en/latest/how_to_guide/16_sbc.html) for more details on SBC."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "560d2b45",
   "metadata": {},
   "source": [
    "### TARP"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7034bad",
   "metadata": {},
   "source": [
    "TARP is an alternative calibration check proposed recently in https://arxiv.org/abs/2302.03026."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "713b9016",
   "metadata": {},
   "outputs": [],
   "source": [
    "ecp, alpha = run_tarp(\n",
    "    thetas,\n",
    "    xs,\n",
    "    posterior,\n",
    "    references=None,  # will be calculated automatically.\n",
    "    num_posterior_samples=10000, # reduce this number to 1000 if you go OOM\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7734950",
   "metadata": {},
   "outputs": [],
   "source": [
    "atc, ks_pval = check_tarp(ecp, alpha)\n",
    "print(atc, \"Should be close to 0\")\n",
    "print(ks_pval, \"Should be larger than 0.05\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc39704d",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_tarp(ecp, alpha)\n",
    "plt.savefig(\"flux1_flow_pipeline_tarp.png\", dpi=100, bbox_inches=\"tight\") # uncomment to save the figure\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84759aef",
   "metadata": {},
   "source": [
    "![flux1_flow_pipeline_tarp.png](https://raw.githubusercontent.com/aurelio-amerio/GenSBI-examples/refs/heads/main/examples/getting_started/flux1_flow_pipeline_tarp.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58678a03",
   "metadata": {},
   "source": [
    "If the blue curve is above the diagonal, then the posterior estimate is under-confident. If it is under the diagonal, then the posterior estimate is over confident.\n",
    "\n",
    "This means that our model is slightly under-confident. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cf09099",
   "metadata": {},
   "source": [
    "See [https://sbi.readthedocs.io/en/latest/how_to_guide/17_tarp.html](https://sbi.readthedocs.io/en/latest/how_to_guide/17_tarp.html) for more details on TARP."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f221201",
   "metadata": {},
   "source": [
    "### L-C2ST"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bddfa5d",
   "metadata": {},
   "source": [
    "Tests like expected coverage and simulation-based calibration evaluate whether the posterior is on average across many observations well-calibrated. Unlike these tools, L-C2ST allows you to evaluate whether the posterior is correct for a specific observation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1678f49",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulate calibration data. Should be at least in the thousands.\n",
    "\n",
    "key = jax.random.PRNGKey(1234)\n",
    "# sample the dataset\n",
    "test_data = simulator(jax.random.PRNGKey(1), 10_000)\n",
    "\n",
    "# split in thetas and xs\n",
    "thetas = test_data[:, :obs_dim, :] # (10_000, 3, 1)\n",
    "xs = test_data[:, obs_dim:, :] # (10_000, 3, 1)\n",
    "\n",
    "# flatten the dataset. sbi expects 2D arrays of shape (num_samples, features), while our data is 3D of shape (num_samples, dim, channels).\n",
    "# we reshape a sample of size (dim, channels) into a vector of size (dim * channels)\n",
    "thetas = posterior._ravel(thetas) # (10_000, 3)\n",
    "xs = posterior._ravel(xs) # (10_000, 3)\n",
    "\n",
    "# convert to torch tensors\n",
    "thetas = torch.Tensor(np.array(thetas))\n",
    "xs = torch.Tensor(np.array(xs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0ca8658",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate one posterior sample for every prior predictive.\n",
    "posterior_samples = posterior.sample_batched(\n",
    "    (1,),\n",
    "    x=xs,\n",
    ")[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02a1cadb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sbi.diagnostics.lc2st import LC2ST\n",
    "\n",
    "\n",
    "# Train the L-C2ST classifier.\n",
    "lc2st = LC2ST(\n",
    "    thetas=thetas,\n",
    "    xs=xs,\n",
    "    posterior_samples=posterior_samples,\n",
    "    classifier=\"mlp\",\n",
    "    num_ensemble=1,\n",
    ")\n",
    "_ = lc2st.train_under_null_hypothesis()\n",
    "_ = lc2st.train_on_observed_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6dd5658",
   "metadata": {},
   "outputs": [],
   "source": [
    "key = jax.random.PRNGKey(12345)\n",
    "\n",
    "sample = simulator(key, 1)\n",
    "theta_true = sample[:, :obs_dim, :]  \n",
    "x_o = sample[:, obs_dim:, :]  \n",
    "\n",
    "# Note: x_o must have a batch-dimension. I.e. `x_o.shape == (1, observation_shape)`.\n",
    "post_samples_star = pipeline.sample(key, x_o, nsamples=10_000) \n",
    "\n",
    "post_samples_star = np.array(post_samples_star.reshape(-1, obs_dim))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddc7bec1",
   "metadata": {},
   "outputs": [],
   "source": [
    "post_samples_star_torch = torch.Tensor(np.array(post_samples_star.reshape(-1, obs_dim)))\n",
    "x_o_torch = torch.Tensor(np.array(x_o.reshape(-1, cond_dim)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbc794f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "probs_data, scores_data = lc2st.get_scores(\n",
    "    theta_o=post_samples_star_torch,\n",
    "    x_o=x_o_torch,\n",
    "    return_probs=True,\n",
    "    trained_clfs=lc2st.trained_clfs\n",
    ")\n",
    "probs_null, scores_null = lc2st.get_statistics_under_null_hypothesis(\n",
    "    theta_o=post_samples_star_torch,\n",
    "    x_o=x_o_torch,\n",
    "    return_probs=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3755ef95",
   "metadata": {},
   "outputs": [],
   "source": [
    "conf_alpha = 0.05\n",
    "p_value = lc2st.p_value(post_samples_star_torch, x_o_torch)\n",
    "reject = lc2st.reject_test(post_samples_star_torch, x_o_torch, alpha=conf_alpha)\n",
    "\n",
    "fig, ax = plt.subplots(1, 1, figsize=(5, 3))\n",
    "quantiles = np.quantile(scores_null, [0, 1-conf_alpha])\n",
    "ax.hist(scores_null, bins=50, density=True, alpha=0.5, label=\"Null\")\n",
    "ax.axvline(scores_data, color=\"red\", label=\"Observed\")\n",
    "ax.axvline(quantiles[0], color=\"black\", linestyle=\"--\", label=\"95% CI\")\n",
    "ax.axvline(quantiles[1], color=\"black\", linestyle=\"--\")\n",
    "ax.set_xlabel(\"Test statistic\")\n",
    "ax.set_ylabel(\"Density\")\n",
    "ax.set_title(f\"p-value = {p_value:.3f}, reject = {reject}\")\n",
    "plt.savefig(\"flux1_flow_pipeline_lc2st.png\", dpi=100, bbox_inches=\"tight\") # uncomment to save the figure\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b211cf64",
   "metadata": {},
   "source": [
    "![flux1_flow_pipeline_lc2st.png](https://raw.githubusercontent.com/aurelio-amerio/GenSBI-examples/refs/heads/main/examples/getting_started/flux1_flow_pipeline_lc2st.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b6602f1",
   "metadata": {},
   "source": [
    "If the red bar falls outside the two dashed black lines, it indicates that the model's posterior estimates are not well-calibrated at the 95% confidence level and further investigation is required."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40cb32ab",
   "metadata": {},
   "source": [
    "### Conclusions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd5ae3aa",
   "metadata": {},
   "source": [
    "What happened? As it turns out, our simple model trained on a small dataset is not always able to provide well-calibrated posterior estimates. This is because the model was trained on a limited number of simulations with a small batch size and for a short duration. To improve the accuracy of the posterior estimates, it is necessary to train the model with a larger batch size (around `1024` or more) and for a longer period (around `50_000` training steps).\n",
    "\n",
    "The `Flux1` architecture is quite powerful and flexible, but being transformer based requires a longer training time and greatly benefits from larger datasets and large batch sizes to avoid overfitting the training dataset.\n",
    "\n",
    "As an experiment, we retrained the model with a batch size of `1024` for `50_000` training steps, and obtained much better results in terms of posterior calibration. For reference, we provide the pre-trained model checkpoint and training configuration to load directly without retraining."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b15e9c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_dir = os.path.join(os.getcwd(), \"examples/getting_started/checkpoints\")\n",
    "\n",
    "config_path = os.path.join(\n",
    "    os.getcwd(), \"examples/getting_started/config_flow_flux.yaml\"\n",
    ")\n",
    "\n",
    "pipeline = Flux1FlowPipeline.init_pipeline_from_config(\n",
    "        train_dataset_grain,\n",
    "        val_dataset_grain,\n",
    "        obs_dim,\n",
    "        cond_dim,\n",
    "        config_path,\n",
    "        checkpoint_dir,\n",
    "    )\n",
    "\n",
    "pipeline.restore_model(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "426f6f6c",
   "metadata": {},
   "source": [
    "Now try running again the code for the posterior calibration tests, after loading the pre-trained model as shown above. You should get something like this, hinting to a much better calibrated model:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7de8f8e",
   "metadata": {},
   "source": [
    "![flux1_flow_pipeline_marginals_2.png](https://raw.githubusercontent.com/aurelio-amerio/GenSBI-examples/refs/heads/main/examples/getting_started/flux1_flow_pipeline_marginals_2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6ac1ac8",
   "metadata": {},
   "source": [
    "![flux1_flow_pipeline_sbc_2.png](https://raw.githubusercontent.com/aurelio-amerio/GenSBI-examples/refs/heads/main/examples/getting_started/flux1_flow_pipeline_sbc_2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2219a575",
   "metadata": {},
   "source": [
    "![flux1_flow_pipeline_tarp_2.png](https://raw.githubusercontent.com/aurelio-amerio/GenSBI-examples/refs/heads/main/examples/getting_started/flux1_flow_pipeline_tarp_2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5344f0e5",
   "metadata": {},
   "source": [
    "![flux1_flow_pipeline_lc2st_2.png](https://raw.githubusercontent.com/aurelio-amerio/GenSBI-examples/refs/heads/main/examples/getting_started/flux1_flow_pipeline_lc2st_2.png)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gensbi",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
