{
    "cells": [
        {
            "cell_type": "markdown",
            "id": "640892b2",
            "metadata": {},
            "source": [
                "# My First Model"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "d7f88520",
            "metadata": {},
            "source": [
                "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/aurelio-amerio/GenSBI-examples/blob/main/examples/getting_started/my_first_model.ipynb)\n"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "539ae5cb",
            "metadata": {},
            "source": [
                "This guide will walk you through creating and training your first simulation-based inference model using GenSBI. We will cover the essential steps, from defining a simulator to training a neural density estimator."
            ]
        },
        {
            "cell_type": "markdown",
            "id": "7228374e",
            "metadata": {},
            "source": [
                "As a first step, make sure GenSBI is installed in your environment. If you haven't done so yet, please refer to the [Installation Guide](/getting_started/installation) before proceeding, or simply run:"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "b45ff30e",
            "metadata": {},
            "outputs": [],
            "source": [
                "# step 1: install packages\n",
                "# %pip install --quiet  \"GenSBI[cuda12,examples] @ git+https://github.com/aurelio-amerio/GenSBI.git\" "
            ]
        },
        {
            "cell_type": "markdown",
            "id": "ad8df57c",
            "metadata": {},
            "source": [
                "Next, it is convenient to download the GenSBI-examples package, which contains several example notebooks and checkpoints, including this one. You can do so by running:"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "553688b1",
            "metadata": {},
            "outputs": [],
            "source": [
                "# step 2: clone the examples repository\n",
                "# !git clone --depth 1 https://github.com/aurelio-amerio/GenSBI-examples.git "
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "8eac9d3c",
            "metadata": {},
            "outputs": [],
            "source": [
                "# step 3: cd into the examples folder\n",
                "# %cd GenSBI-examples/examples/getting_started/GenSBI-examples"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "ef46258d",
            "metadata": {},
            "outputs": [],
            "source": [
                "# automatically install dependencies if using Colab\n",
                "try: #check if we are using colab, if so install all the required software\n",
                "    import google.colab\n",
                "    colab=True\n",
                "except:\n",
                "    colab=False\n",
                "\n",
                "if colab: # you may have to restart the runtime after installing the packages\n",
                "    %pip install --quiet \"gensbi[cuda12, examples] @ git+https://github.com/aurelio-amerio/GenSBI\"\n",
                "    !git clone --depth 1 https://github.com/aurelio-amerio/GenSBI-examples\n",
                "    %cd GenSBI-examples/examples/getting_started"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "be2f6b93",
            "metadata": {},
            "source": [
                "> **Important**: <br><br>\n",
                "> If you are using Colab, you may need to restart the runtime after installation to ensure all packages are properly loaded. <br>"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "e97c4a04",
            "metadata": {},
            "source": [
                "Import the necessary modules from GenSBI and other libraries. If you don't have a GPU available, set `JAX_PLATFORMS` to \"cpu\" in the cell below, but note that training will be significantly slower.\n",
                "\n",
                "If you encounter import errors after installing, restart the notebook kernel and re-run this cell."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "10ac5993",
            "metadata": {},
            "outputs": [],
            "source": [
                "import os\n",
                "\n",
                "# Set JAX backend (use 'cuda' for GPU, 'cpu' otherwise)\n",
                "# os.environ[\"JAX_PLATFORMS\"] = \"cuda\"\n",
                "os.environ[\"JAX_PLATFORMS\"] = \"cpu\"\n",
                "\n",
                "import grain\n",
                "import numpy as np\n",
                "import jax\n",
                "from jax import numpy as jnp\n",
                "from numpyro import distributions as dist\n",
                "from flax import nnx\n",
                "\n",
                "from gensbi.recipes import Flux1FlowPipeline\n",
                "from gensbi.models import Flux1Params\n",
                "\n",
                "from gensbi.utils.plotting import plot_marginals\n",
                "import matplotlib.pyplot as plt"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "26b0ab07",
            "metadata": {},
            "source": [
                "## The simulator"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "93b2249b",
            "metadata": {},
            "source": [
                "The first step in SBI is defining a **simulator**. The simulator takes input parameters $\theta$ and produces synthetic observations $x$. For this tutorial, we use a simple simulator where the observation $x$ is drawn from a Gaussian distribution centered at $\theta$."
            ]
        },
        {
            "cell_type": "markdown",
            "id": "5ca46c8e",
            "metadata": {},
            "source": [
                "The simulator takes in a parameter vector `theta` of size 3 and returns an observation vector `xs` of size 3. \n",
                "\n",
                "In the context of posterior density estimation (Simulation-Based Inference), our goal is to infer the parameters `theta` given an observation `xs`. Therefore, `theta` is the target variable (what we want to predict the distribution of) and `xs` is the condition."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "a43dcbc8",
            "metadata": {},
            "outputs": [],
            "source": [
                "dim_obs = 3 # dimension of the observation (theta), that is the simulator input shape\n",
                "dim_cond = 3 # dimension of the condition (xs), that is the simulator output shape\n",
                "dim_joint = dim_obs + dim_cond # dimension of the joint (theta, xs), useful later\n",
                "\n",
                "def _simulator(key, thetas):\n",
                "\n",
                "    xs = thetas + 1 + jax.random.normal(key, thetas.shape) * 0.1\n",
                "\n",
                "    thetas = thetas[..., None]\n",
                "    xs = xs[..., None]\n",
                "\n",
                "    # when making a dataset for the joint pipeline, thetas need to come first\n",
                "    data = jnp.concatenate([thetas, xs], axis=1)\n",
                "\n",
                "    return data"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "dcebf82a",
            "metadata": {},
            "source": [
                "Next, we define a **prior distribution** $p(\theta)$, which represents our knowledge about the parameters before observing any data. Here, we use a Uniform prior."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "9fedf995",
            "metadata": {},
            "outputs": [],
            "source": [
                "theta_prior = dist.Uniform(\n",
                "    low=jnp.array([-2.0, -2.0, -2.0]), high=jnp.array([2.0, 2.0, 2.0])\n",
                ")"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "6ee0f0aa",
            "metadata": {},
            "source": [
                "For convenience, we define a wrapper function that handles both prior sampling and data generation in a single call."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "2d937abc",
            "metadata": {},
            "outputs": [],
            "source": [
                "def simulator(key, nsamples):\n",
                "    theta_key, sample_key = jax.random.split(key, 2)\n",
                "    thetas = theta_prior.sample(theta_key, (nsamples,))\n",
                "\n",
                "    return _simulator(sample_key, thetas)"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "700b7ccf",
            "metadata": {},
            "source": [
                "## The dataset"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "ef9dcc67",
            "metadata": {},
            "source": [
                "We generate a training dataset by running the simulator multiple times. We sample parameters from the prior and then run the simulator for each parameter set. This dataset of $(\theta, x)$ pairs is used to train the neural density estimator.\n",
                "\n",
                "GenSBI is designed to work with any dataset that provides an iterator yielding pairs of (parameters, observations). \n",
                "However, for efficient training, especially with large datasets, we recommend using a high-performance data loader like `grain` to handle batching, shuffling, and prefetching."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "39505671",
            "metadata": {},
            "outputs": [],
            "source": [
                "# Define your training and validation datasets.\n",
                "train_data = simulator(jax.random.PRNGKey(0), 100_000)\n",
                "val_data = simulator(jax.random.PRNGKey(1), 2000)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "3fd2a51c",
            "metadata": {},
            "outputs": [],
            "source": [
                "# utility function to split data into observations and conditions\n",
                "def split_obs_cond(data):\n",
                "    return data[:, :dim_obs], data[:, dim_obs:]  # assuming first dim_obs are obs, last dim_cond are cond"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "32f34cd8",
            "metadata": {},
            "source": [
                "We create a `grain` dataset with batch size = 256. The larger the batch size, the more stable the training.\n",
                "\n",
                "Adjust according to your hardware capabilities, e.g. GPU memory (try experimenting with 256, 512, 1024, etc).\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "5f406808",
            "metadata": {},
            "outputs": [],
            "source": [
                "batch_size = 256\n",
                "\n",
                "train_dataset_grain = (\n",
                "    grain.MapDataset.source(np.array(train_data))\n",
                "    .shuffle(42)\n",
                "    .repeat()\n",
                "    .to_iter_dataset()\n",
                "    .batch(batch_size)\n",
                "    .map(split_obs_cond)\n",
                "    .mp_prefetch() # If you use prefetching in a .py script, make sure your python script is thread safe, see https://docs.python.org/3/library/multiprocessing.html\n",
                ")\n",
                "\n",
                "val_dataset_grain = (\n",
                "    grain.MapDataset.source(np.array(val_data))\n",
                "    .shuffle(42)\n",
                "    .repeat()\n",
                "    .to_iter_dataset()\n",
                "    .batch(batch_size)\n",
                "    .map(split_obs_cond)\n",
                "    .mp_prefetch() \n",
                ")"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "8c678015",
            "metadata": {},
            "source": [
                "Because we called `.repeat()`, these dataloaders cycle through the data indefinitely, which is required for step-based training.\n",
                "You can get samples from the dataset using:"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "ba1af81a",
            "metadata": {},
            "outputs": [],
            "source": [
                "iter_dataset = iter(train_dataset_grain)\n",
                "obs,cond = next(iter_dataset)  # returns a batch of (observations, conditions)\n",
                "print(obs.shape, cond.shape)  # should print (batch_size, dim_obs, 1), (batch_size, dim_cond, 1)"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "c5ac9e7d",
            "metadata": {},
            "source": [
                "## The Model"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "67ded857",
            "metadata": {},
            "source": [
                "We now set up the **Neural Density Estimator**. We use `Flux1`, a state-of-the-art transformer-based flow matching model. While this architecture is overkill for a simple Gaussian problem, we use it here to demonstrate the standard workflow for complex tasks."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "f3f352d2",
            "metadata": {},
            "outputs": [],
            "source": [
                "# define the model parameters\n",
                "params = Flux1Params(\n",
                "    in_channels=1,  # each observation/condition feature has only one channel (the value itself)\n",
                "    vec_in_dim=None,\n",
                "    context_in_dim=1,\n",
                "    mlp_ratio=3,  # default value\n",
                "    num_heads=4,  # number of transformer heads\n",
                "    depth=4,  # number of double-stream transformer blocks\n",
                "    depth_single_blocks=8,  # number of single-stream transformer blocks\n",
                "    val_emb_dim=10,  # Features per head for value embedding\n",
                "    id_emb_dim=4,  # Features per head for ID embedding\n",
                "    qkv_bias=True,  # default\n",
                "    dim_obs=dim_obs,  # dimension of the observation (theta)\n",
                "    dim_cond=dim_cond,  # dimension of the condition (xs)\n",
                "    id_merge_mode=\"concat\",\n",
                "    id_embedding_strategy=(\"absolute\", \"absolute\"),\n",
                "    rngs=nnx.Rngs(default=42),  # random number generator seed\n",
                "    param_dtype=jnp.bfloat16,  # data type of the model parameters. if bfloat16 is not available on your machine, use float32\n",
                ")\n",
                "\n",
                "\n",
                "# you can also try the \"sum\" embedding strategy, how does the performance of the model compare? Why? Hint: this is a low dimensional problem, with small axes_dim\n",
                "# params = Flux1Params(\n",
                "#     in_channels=1,  # each observation/condition feature has only one channel (the value itself)\n",
                "#     vec_in_dim=None,\n",
                "#     context_in_dim=1,\n",
                "#     mlp_ratio=3,  # default value\n",
                "#     num_heads=2,  # number of transformer heads\n",
                "#     depth=4,  # number of double-stream transformer blocks\n",
                "#     depth_single_blocks=8,  # number of single-stream transformer blocks\n",
                "#     axes_dim = [10], # Features per head for value embedding\n",
                "#     qkv_bias=True,  # default\n",
                "#     dim_obs=dim_obs,  # dimension of the observation (theta)\n",
                "#     dim_cond=dim_cond,  # dimension of the condition (xs)\n",
                "#     id_merge_mode=\"sum\",\n",
                "#     id_embedding_strategy=(\"absolute\", \"absolute\"),\n",
                "#     rngs=nnx.Rngs(default=42),  # random number generator seed\n",
                "#     param_dtype=jnp.bfloat16,  # data type of the model parameters. if bfloat16 is not available on your machine, use float32\n",
                "# )"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "60cc6d7f",
            "metadata": {},
            "source": [
                "Next, we configure the training hyperparameters. We start from the default training configuration and customize a few key settings:"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "13c5b558",
            "metadata": {},
            "outputs": [],
            "source": [
                "checkpoint_dir = f\"{os.getcwd()}/checkpoints\"\n",
                "\n",
                "training_config = Flux1FlowPipeline.get_default_training_config()\n",
                "training_config[\"checkpoint_dir\"] = checkpoint_dir\n",
                "training_config[\"experiment_id\"] = 1\n",
                "training_config[\"nsteps\"] = 10_000\n",
                "training_config[\"decay_transition\"] = 0.80\n",
                "training_config[\"warmup_steps\"] = 500"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "7268f0ab",
            "metadata": {},
            "source": [
                " > **Note:**<br>\n",
                " > It is important to set the number of training steps (`nsteps`) in the training config, as this will ensure warmup steps and decay transition are computed correctly.  "
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "bb5b6f67",
            "metadata": {},
            "outputs": [],
            "source": [
                "# Instantiate the pipeline\n",
                "pipeline = Flux1FlowPipeline(\n",
                "    train_dataset_grain,\n",
                "    val_dataset_grain,\n",
                "    dim_obs,\n",
                "    dim_cond,\n",
                "    params=params,\n",
                "    training_config=training_config,\n",
                ")"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "e67fc0ab",
            "metadata": {},
            "source": [
                "## Training"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "a7f242be",
            "metadata": {},
            "source": [
                "Now we train the model. The number of training steps was already set in the training configuration above. We only need to provide a random number generator for reproducibility."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "8ea17843",
            "metadata": {},
            "outputs": [],
            "source": [
                "rngs = nnx.Rngs(42)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "521a5ec3",
            "metadata": {},
            "outputs": [],
            "source": [
                "# uncomment to train the model\n",
                "loss_history = pipeline.train(\n",
                "    rngs, save_model=False\n",
                ")  # if you want to save the model, set save_model=True"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "76273927",
            "metadata": {},
            "source": [
                "Alternatively, you can skip training and load the pre-trained checkpoint provided with this example:"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "97bbb461",
            "metadata": {},
            "outputs": [],
            "source": [
                "# pipeline.restore_model(2) # we have stored the pretrained model with tag 2"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "8751b1eb",
            "metadata": {},
            "outputs": [],
            "source": [
                "steps = np.linspace(1, len(loss_history[0]), len(loss_history[0]))*100\n",
                "plt.plot(steps, loss_history[0], label=\"train loss\")\n",
                "plt.plot(steps, loss_history[1], label=\"val loss\")\n",
                "plt.yscale(\"log\")\n",
                "plt.xlabel(\"steps\")\n",
                "plt.ylabel(\"loss\")\n",
                "plt.ylim(0.1,10)\n",
                "plt.legend()\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "2a233140",
            "metadata": {},
            "source": [
                "<img src=\"https://raw.githubusercontent.com/aurelio-amerio/GenSBI-examples/refs/heads/main/examples/getting_started/flux1_flow_pipeline_loss_2.png\" width=400>"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "a8036c1c",
            "metadata": {},
            "source": [
                "After the training is complete, by inspecting the loss curve we can see that the model has converged to a stable value for the train and validation loss.\n",
                "\n",
                "Note that, unlike traditional tasks, flow and diffusion models keep \"learning\" even when the loss function seems to have stabilized. As such, even though the loss function seems to have stabilized after the scheduled training steps, it is often beneficial to keep training the model for longer. \n",
                "\n",
                "Flow and diffusion models are less likely to overfit the training data, given their stochastic nature. Nonetheless, if the model is excessively over-parameterized, and not enough training data is provided, artifacts in the posteriors may appear in the form of \"spikes\".\n",
                "On the other hand, if the model is under-parameterized, the posterior may be excessively smooth, or underconfident. "
            ]
        },
        {
            "cell_type": "markdown",
            "id": "02560e97",
            "metadata": {},
            "source": [
                "## Sampling from the posterior"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "0f8fbffe",
            "metadata": {},
            "source": [
                "Once the model is trained, we can estimate the posterior distribution for any new observation. We pass the observed data to the pipeline's `sample` method, which draws samples from the learned posterior."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "d904ee23",
            "metadata": {},
            "outputs": [],
            "source": [
                "new_sample = simulator(jax.random.PRNGKey(20), 1) # generate one (theta, x) pair\n",
                "\n",
                "true_theta = new_sample[:, :dim_obs, :]  # the true parameters used for the simulation\n",
                "x_o = new_sample[:, dim_obs:, :]  # the observed data, which we condition on"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "b57a751e",
            "metadata": {},
            "source": [
                "Now we sample from the posterior:"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "84c45bd2",
            "metadata": {},
            "outputs": [],
            "source": [
                "samples = pipeline.sample(rngs.sample(), x_o, nsamples=100_000)"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "36eff93a",
            "metadata": {},
            "source": [
                "Once we have the samples, we display the marginal distributions:"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "45f1e724",
            "metadata": {},
            "outputs": [],
            "source": [
                "plot_marginals(\n",
                "    np.array(samples[..., 0]), gridsize=30, true_param=np.array(true_theta[0, :, 0]), range = [(1, 3), (1, 3), (-0.6, 0.5)]\n",
                ")\n",
                "# plt.savefig(\"flux1_flow_pipeline_marginals.png\", dpi=100, bbox_inches=\"tight\") # uncomment to save the figure\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "6a18204b",
            "metadata": {},
            "source": [
                "<img src=\"https://raw.githubusercontent.com/aurelio-amerio/GenSBI-examples/refs/heads/main/examples/getting_started/flux1_flow_pipeline_marginals_2.png\" width=600>"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "f50b90c2",
            "metadata": {},
            "source": [
                "## Next steps"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "461d3864",
            "metadata": {},
            "source": [
                "Congratulations! You have successfully created and trained your first simulation-based inference model using GenSBI. You can now experiment with different simulators, priors, and neural density estimators to explore more complex inference tasks.\n",
                "\n",
                "For more examples, please refer to the [Examples Section](/examples) of the GenSBI documentation.\n",
                "\n",
                "As a next step, you might want to explore how to validate the performance of your trained model using techniques such as simulation-based calibration (SBC) or coverage plots. These methods help assess the quality of the inferred posterior distributions and ensure that your model is providing accurate uncertainty estimates."
            ]
        },
        {
            "cell_type": "markdown",
            "id": "42900c2b",
            "metadata": {},
            "source": [
                "## Posterior calibration tests"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "913cf3b5",
            "metadata": {},
            "source": [
                "In this section, we perform posterior calibration tests using Simulation-Based Calibration (SBC), Targeted At Random Parameters (TARP) and L-C2ST methods to evaluate the quality of our trained model's posterior estimates."
            ]
        },
        {
            "cell_type": "markdown",
            "id": "ba10e64d",
            "metadata": {},
            "source": [
                "For a full overview of posterior calibration tests, refer to the [sbi documentation](https://sbi.readthedocs.io/en/latest/how_to_guide.html#diagnostics). "
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "3ddd0c9b",
            "metadata": {},
            "outputs": [],
            "source": [
                "# imports\n",
                "from gensbi.diagnostics import check_tarp, run_tarp, plot_tarp\n",
                "from gensbi.diagnostics import check_sbc, run_sbc, sbc_rank_plot\n",
                "from gensbi.diagnostics import LC2ST, plot_lc2st"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "b6702c6a",
            "metadata": {},
            "source": [
                "We sample 200 new observations from the simulator to perform the calibration tests.\n",
                "It is crucial that we use a seed different from the one used during training to avoid biased results."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "26468d52",
            "metadata": {},
            "outputs": [],
            "source": [
                "key = jax.random.PRNGKey(1234)\n",
                "# sample the dataset\n",
                "test_data_ = simulator(jax.random.PRNGKey(1), 200)\n",
                "\n",
                "\n",
                "# split in thetas and xs\n",
                "thetas_ = test_data[:, :dim_obs, :] # (200, 3, 1)\n",
                "xs_ = test_data[:, dim_obs:, :] # (200, 3, 1)\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "29813af5",
            "metadata": {},
            "outputs": [],
            "source": [
                "# sample the posterior for each observation in xs_\n",
                "posterior_samples_ = pipeline.sample_batched(jax.random.PRNGKey(0), xs_, nsamples=1000)  # (1000, 200, 3, 1)"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "357ea6ea",
            "metadata": {},
            "source": [
                "For the sake of posterior calibration tests, the last two dimensions need to be flattened into a single dimension."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "10e3042f",
            "metadata": {},
            "outputs": [],
            "source": [
                "thetas = thetas_.reshape(thetas_.shape[0], -1)  # (200, 3)\n",
                "xs = xs_.reshape(xs_.shape[0], -1)  # (200, 3)\n",
                "posterior_samples = posterior_samples_.reshape(posterior_samples_.shape[0], posterior_samples_.shape[1], -1)  # (1000, 200, 3)"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "56f1ad74",
            "metadata": {},
            "source": [
                "### SBC"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "ada81029",
            "metadata": {},
            "source": [
                "SBC checks whether the individual marginal posteriors are well-calibrated on average across many observations. It can reveal if the posteriors are systematically too narrow, too wide, or skewed."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "ee959adc",
            "metadata": {},
            "outputs": [],
            "source": [
                "ranks, dap_samples = run_sbc(thetas, xs, posterior_samples)\n",
                "check_stats = check_sbc(ranks, thetas, dap_samples, 1_000)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "659ce692",
            "metadata": {},
            "outputs": [],
            "source": [
                "print(check_stats)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "efb2620f",
            "metadata": {},
            "outputs": [],
            "source": [
                "f, ax = sbc_rank_plot(ranks, 1_000, plot_type=\"hist\", num_bins=20)\n",
                "plt.savefig(\"flux1_flow_pipeline_sbc.png\", dpi=100, bbox_inches=\"tight\") # uncomment to save the figure\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "26f5d683",
            "metadata": {},
            "source": [
                "<img src=\"https://raw.githubusercontent.com/aurelio-amerio/GenSBI-examples/refs/heads/main/examples/getting_started/flux1_flow_pipeline_sbc_2.png\" width=600>"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "9ea5098e",
            "metadata": {},
            "source": [
                "All of the bars fall within the confidence intervals of the uniform distribution, thus we cannot reject the hypothesis that the posterior marginals are calibrated."
            ]
        },
        {
            "cell_type": "markdown",
            "id": "de1b8d73",
            "metadata": {},
            "source": [
                "See the SBI tutorial [https://sbi.readthedocs.io/en/latest/how_to_guide/16_sbc.html](https://sbi.readthedocs.io/en/latest/how_to_guide/16_sbc.html) for more details on SBC."
            ]
        },
        {
            "cell_type": "markdown",
            "id": "560d2b45",
            "metadata": {},
            "source": [
                "### TARP"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "f7034bad",
            "metadata": {},
            "source": [
                "TARP is an alternative calibration check that evaluates the joint posterior (not just individual marginals). See [Lemos et al. (2023)](https://arxiv.org/abs/2302.03026) for details."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "713b9016",
            "metadata": {},
            "outputs": [],
            "source": [
                "ecp, alpha = run_tarp(\n",
                "    thetas,\n",
                "    posterior_samples,\n",
                "    references=None,  # will be calculated automatically.\n",
                ")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "a7734950",
            "metadata": {},
            "outputs": [],
            "source": [
                "atc, ks_pval = check_tarp(ecp, alpha)\n",
                "print(atc, \"Should be close to 0\")\n",
                "print(ks_pval, \"Should be larger than 0.05\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "fc39704d",
            "metadata": {},
            "outputs": [],
            "source": [
                "plot_tarp(ecp, alpha)\n",
                "plt.savefig(\"flux1_flow_pipeline_tarp.png\", dpi=100, bbox_inches=\"tight\") # uncomment to save the figure\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "84759aef",
            "metadata": {},
            "source": [
                "<img src=\"https://raw.githubusercontent.com/aurelio-amerio/GenSBI-examples/refs/heads/main/examples/getting_started/flux1_flow_pipeline_tarp_2.png\" width=400>"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "58678a03",
            "metadata": {},
            "source": [
                "If the blue curve is above the diagonal, then the posterior estimate is under-confident. If it is under the diagonal, then the posterior estimate is over-confident.\n",
                "\n",
                "While the curve does not coincide exactly with the diagonal, from the TARP test we cannot reject the hypothesis that the model is properly calibrated."
            ]
        },
        {
            "cell_type": "markdown",
            "id": "1cf09099",
            "metadata": {},
            "source": [
                "See [https://sbi.readthedocs.io/en/latest/how_to_guide/17_tarp.html](https://sbi.readthedocs.io/en/latest/how_to_guide/17_tarp.html) for more details on TARP."
            ]
        },
        {
            "cell_type": "markdown",
            "id": "6f221201",
            "metadata": {},
            "source": [
                "### L-C2ST"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "3bddfa5d",
            "metadata": {},
            "source": [
                "Unlike SBC and TARP, which evaluate average calibration across many observations, L-C2ST tests whether the posterior is accurate for a *specific* observation. This makes it useful for diagnosing local failures."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "e1678f49",
            "metadata": {},
            "outputs": [],
            "source": [
                "# Simulate calibration data. Should be at least in the thousands.\n",
                "\n",
                "key = jax.random.PRNGKey(1234)\n",
                "# sample the dataset\n",
                "test_data = simulator(jax.random.PRNGKey(1), 10_000)\n",
                "\n",
                "# split in thetas and xs\n",
                "thetas_ = test_data[:, :dim_obs, :] # (10_000, 3, 1)\n",
                "xs_ = test_data[:, dim_obs:, :] # (10_000, 3, 1)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "e0ca8658",
            "metadata": {},
            "outputs": [],
            "source": [
                "# Generate one posterior sample for every prior predictive.\n",
                "posterior_samples_ = pipeline.sample(key, x_o=xs_, nsamples=xs_.shape[0])"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "ad7917f1",
            "metadata": {},
            "outputs": [],
            "source": [
                "thetas = thetas_.reshape(thetas_.shape[0], -1)  # (10_000, 3)\n",
                "xs = xs_.reshape(xs_.shape[0], -1)  # (10_000, 3)\n",
                "posterior_samples = posterior_samples_.reshape(posterior_samples_.shape[0], -1)  # (10_000, 3)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "02a1cadb",
            "metadata": {},
            "outputs": [],
            "source": [
                "from gensbi.diagnostics import LC2ST, plot_lc2st\n",
                "\n",
                "\n",
                "# Train the L-C2ST classifier.\n",
                "lc2st = LC2ST(\n",
                "    thetas=thetas,\n",
                "    xs=xs,\n",
                "    posterior_samples=posterior_samples,\n",
                "    classifier=\"mlp\",\n",
                "    num_ensemble=1,\n",
                ")\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "5363771b",
            "metadata": {},
            "outputs": [],
            "source": [
                "_ = lc2st.train_under_null_hypothesis()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "6bfc4963",
            "metadata": {},
            "outputs": [],
            "source": [
                "_ = lc2st.train_on_observed_data()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "f6dd5658",
            "metadata": {},
            "outputs": [],
            "source": [
                "key = jax.random.PRNGKey(12345)\n",
                "\n",
                "sample = simulator(key, 1)\n",
                "# theta_true_ = sample[:, :dim_obs, :]  \n",
                "x_o_ = sample[:, dim_obs:, :]  \n",
                "\n",
                "# Note: x_o must have a batch-dimension. I.e. `x_o.shape == (1, observation_shape)`.\n",
                "post_samples_star_ = pipeline.sample(key, x_o_, nsamples=10_000) "
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "326147cb",
            "metadata": {},
            "outputs": [],
            "source": [
                "# theta_true = theta_true_.reshape(-1)  # (3,)\n",
                "x_o = x_o_.reshape(1,-1)  # (3,)\n",
                "post_samples_star = np.array(post_samples_star_.reshape(post_samples_star_.shape[0], -1))  # (10_000, 3)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "a942dddb",
            "metadata": {},
            "outputs": [],
            "source": [
                "post_samples_star.shape, x_o.shape"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "c1d6cbb7",
            "metadata": {},
            "outputs": [],
            "source": [
                "fig,ax = plot_lc2st(\n",
                "    lc2st,\n",
                "    post_samples_star,\n",
                "    x_o,\n",
                ")\n",
                "plt.savefig(\"flux1_flow_pipeline_lc2st.png\", dpi=100, bbox_inches=\"tight\") # uncomment to save the figure\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "b211cf64",
            "metadata": {},
            "source": [
                "<img src=\"https://raw.githubusercontent.com/aurelio-amerio/GenSBI-examples/refs/heads/main/examples/getting_started/flux1_flow_pipeline_lc2st_2.png\" width=600>"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "0b6602f1",
            "metadata": {},
            "source": [
                "If the red bar falls outside the two dashed black lines, it indicates that the model's posterior estimates are not well-calibrated at the 95% confidence level and further investigation is required.\n",
                "\n",
                "For the specific chosen observation, the model seems to be properly calibrated."
            ]
        },
        {
            "cell_type": "markdown",
            "id": "40cb32ab",
            "metadata": {},
            "source": [
                "## Conclusions"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "cd5ae3aa",
            "metadata": {},
            "source": [
                "Based on SBC, TARP, and L-C2ST, all calibration tests are consistent with a well-calibrated posterior. We cannot reject the hypothesis that the model is properly calibrated."
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "gensbi",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.12.11"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 5
}
